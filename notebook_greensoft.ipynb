{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "VPIBYi6koqvF",
        "outputId": "34ee13ae-fcc0-49e7-c8ac-3bebb2b6499f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Optimizing User Code (Green LLM) ===\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AuthenticationError",
          "evalue": "Error code: 401 - {'status': 401, 'title': 'Unauthorized', 'detail': 'Authentication failed'}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAuthenticationError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-520647253.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1895\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1896\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n=== Optimizing User Code (Green LLM) ===\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1897\u001b[0;31m     \u001b[0msuggestion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuggest_optimization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1898\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\\n=== Final Suggestion ===\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1899\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuggestion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-520647253.py\u001b[0m in \u001b[0;36msuggest_optimization\u001b[0;34m(user_code)\u001b[0m\n\u001b[1;32m   1670\u001b[0m     \u001b[0manalysis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manalyze_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1671\u001b[0m     \u001b[0;31m# Step B: Retrieve KB rules (sanitized)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1672\u001b[0;31m     \u001b[0mbest_rules\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mretrieve_best_practices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1673\u001b[0m     \u001b[0;31m# Step C: Build RAG prompt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1674\u001b[0m     rag_prompt = f\"\"\"\n",
            "\u001b[0;32m/tmp/ipython-input-520647253.py\u001b[0m in \u001b[0;36mretrieve_best_practices\u001b[0;34m(code, top_k)\u001b[0m\n\u001b[1;32m   1539\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mretrieve_best_practices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_k\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m     \u001b[0;31m# Compute similarity on-the-fly; don't rely on stored vectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1541\u001b[0;31m     \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membed_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"query\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m     \u001b[0mscored\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mknowledge_base\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-520647253.py\u001b[0m in \u001b[0;36membed_text\u001b[0;34m(text, input_type)\u001b[0m\n\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0membed_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_type\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"query\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1514\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_embed_cached\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1516\u001b[0m \u001b[0;31m# Embed KB entries as \"passages\" (kept for compatibility, but gated)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-520647253.py\u001b[0m in \u001b[0;36m_embed_cached\u001b[0;34m(text, input_type)\u001b[0m\n\u001b[1;32m   1503\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mlru_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4096\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1504\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_embed_cached\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_type\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"query\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1505\u001b[0;31m     emb = client.embeddings.create(\n\u001b[0m\u001b[1;32m   1506\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"nvidia/nv-embedqa-e5-v5\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1507\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/resources/embeddings.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, input, model, dimensions, encoding_format, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    133\u001b[0m             \u001b[0;34m\"/embeddings\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaybe_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_create_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbeddingCreateParams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1257\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1258\u001b[0m         )\n\u001b[0;32m-> 1259\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1261\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m                 \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1047\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1049\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAuthenticationError\u001b[0m: Error code: 401 - {'status': 401, 'title': 'Unauthorized', 'detail': 'Authentication failed'}"
          ]
        }
      ],
      "source": [
        "import json\n",
        "from openai import OpenAI\n",
        "from typing import List, Dict\n",
        "import ast\n",
        "import re\n",
        "import numpy as np\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 0. Configure NVIDIA NIM client\n",
        "# ------------------------------------------------------------\n",
        "client = OpenAI(\n",
        "    base_url=\"https://integrate.api.nvidia.com/v1\",\n",
        "    api_key=\"NVIDIA API KEY: https://build.nvidia.com/models\"\n",
        ")\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 1. Brique 1 – Base de connaissances (static KB)\n",
        "# Rules (expandable from GitHub CAST / cnumr best practices, all turned to Python for consistency reasons)\n",
        "# ------------------------------------------------------------\n",
        "knowledge_base = [\n",
        "  {\n",
        "    \"id\": \"rule_001\",\n",
        "    \"langage\": \"Python\",\n",
        "    \"contexte\": \"Algorithmes et boucles\",\n",
        "    \"anti_pattern\": \"Boucles Python inefficaces pour transformer des données\",\n",
        "    \"advice\": \"Utiliser des compréhensions de liste ou des générateurs au lieu de for-loops manuels lorsque c'est possible.\",\n",
        "    \"exemple_avant\": \"res = []\\nfor x in data:\\n    res.append(x * x)\",\n",
        "    \"exemple_apres\": \"res = [x * x for x in data]\",\n",
        "    \"gain_cpu\": \"-30%\",\n",
        "    \"gain_ram\": \"-10%\",\n",
        "    \"outil_mesure\": [\"memory_profiler\", \"timeit\"],\n",
        "    \"estimation_co2\": \"≈0.10g CO₂ économisés pour 1M opérations\"\n",
        "  },\n",
        "  {\n",
        "    \"id\": \"rule_002\",\n",
        "    \"langage\": \"Python\",\n",
        "    \"contexte\": \"Structures de données\",\n",
        "    \"anti_pattern\": \"Recherche linéaire dans une liste à l'intérieur d'une boucle\",\n",
        "    \"advice\": \"Utiliser un set ou un dictionnaire pour des tests d'appartenance en O(1) au lieu de scans en O(n).\",\n",
        "    \"exemple_avant\": \"hits = 0\\nfor x in items:\\n    if x in big_list:\\n        hits += 1\",\n",
        "    \"exemple_apres\": \"big_set = set(big_list)\\nhits = sum(1 for x in items if x in big_set)\",\n",
        "    \"gain_cpu\": \"-60%\",\n",
        "    \"gain_ram\": \"-20%\",\n",
        "    \"outil_mesure\": [\"memory_profiler\", \"timeit\"],\n",
        "    \"estimation_co2\": \"≈0.22g CO₂ économisés pour 1M opérations\"\n",
        "  },\n",
        "  {\n",
        "    \"id\": \"rule_003\",\n",
        "    \"langage\": \"Python\",\n",
        "    \"contexte\": \"Chaînes de caractères\",\n",
        "    \"anti_pattern\": \"Concaténation de chaînes dans une boucle\",\n",
        "    \"advice\": \"Construire une liste puis utiliser ''.join(list) pour assembler la chaîne.\",\n",
        "    \"exemple_avant\": \"s = \\\"\\\"\\nfor part in parts:\\n    s += part\",\n",
        "    \"exemple_apres\": \"s = \\\"\\\".join(parts)\",\n",
        "    \"gain_cpu\": \"-70%\",\n",
        "    \"gain_ram\": \"-20%\",\n",
        "    \"outil_mesure\": [\"memory_profiler\", \"timeit\"],\n",
        "    \"estimation_co2\": \"≈0.24g CO₂ économisés pour 1M opérations\"\n",
        "  },\n",
        "  {\n",
        "    \"id\": \"rule_004\",\n",
        "    \"langage\": \"Python\",\n",
        "    \"contexte\": \"Algorithmes\",\n",
        "    \"anti_pattern\": \"Tri répété dans une boucle\",\n",
        "    \"advice\": \"Trier une seule fois ou utiliser heapq pour des extractions partielles.\",\n",
        "    \"exemple_avant\": \"for x in batches:\\n    data.sort()\\n    use(data[0])\",\n",
        "    \"exemple_apres\": \"import heapq\\nheap = data[:]\\nheapq.heapify(heap)\\nfor x in batches:\\n    use(heapq.heappop(heap))\",\n",
        "    \"gain_cpu\": \"-60%\",\n",
        "    \"gain_ram\": \"-20%\",\n",
        "    \"outil_mesure\": [\"memory_profiler\", \"timeit\"],\n",
        "    \"estimation_co2\": \"≈0.22g CO₂ économisés pour 1M opérations\"\n",
        "  },\n",
        "  {\n",
        "    \"id\": \"rule_005\",\n",
        "    \"langage\": \"Python\",\n",
        "    \"contexte\": \"Fichiers et I/O\",\n",
        "    \"anti_pattern\": \"Ouverture/fermeture de fichiers manuelle sans gestion de contexte\",\n",
        "    \"advice\": \"Utiliser des gestionnaires de contexte (with open(...)) pour une gestion sûre et efficace.\",\n",
        "    \"exemple_avant\": \"f = open(path, \\\"w\\\")\\nfor row in rows:\\n    f.write(row)\\nf.close()\",\n",
        "    \"exemple_apres\": \"with open(path, \\\"w\\\") as f:\\n    for row in rows:\\n        f.write(row)\",\n",
        "    \"gain_cpu\": \"-10%\",\n",
        "    \"gain_ram\": \"-10%\",\n",
        "    \"outil_mesure\": [\"memory_profiler\", \"timeit\"],\n",
        "    \"estimation_co2\": \"≈0.05g CO₂ économisés pour 1M écritures\"\n",
        "  },\n",
        "  {\n",
        "    \"id\": \"rule_006\",\n",
        "    \"langage\": \"Python\",\n",
        "    \"contexte\": \"Regex et parsing\",\n",
        "    \"anti_pattern\": \"Compilation répétée d'expressions régulières\",\n",
        "    \"advice\": \"Compiler une fois avec re.compile() et réutiliser le motif.\",\n",
        "    \"exemple_avant\": \"import re\\nfor line in lines:\\n    if re.search(r\\\"\\\\d+\\\", line):\\n        process(line)\",\n",
        "    \"exemple_apres\": \"import re\\npat = re.compile(r\\\"\\\\d+\\\")\\nfor line in lines:\\n    if pat.search(line):\\n        process(line)\",\n",
        "    \"gain_cpu\": \"-40%\",\n",
        "    \"gain_ram\": \"-10%\",\n",
        "    \"outil_mesure\": [\"memory_profiler\", \"timeit\"],\n",
        "    \"estimation_co2\": \"≈0.12g CO₂ économisés pour 1M lignes\"\n",
        "  },\n",
        "  {\n",
        "    \"id\": \"rule_007\",\n",
        "    \"langage\": \"Python\",\n",
        "    \"contexte\": \"Qualité de code\",\n",
        "    \"anti_pattern\": \"Variables inutilisées\",\n",
        "    \"advice\": \"Supprimer les variables inutiles pour réduire l'empreinte mémoire et clarifier le code.\",\n",
        "    \"exemple_avant\": \"tmp = heavy_calc()\\n_ = 0\\nreturn tmp\",\n",
        "    \"exemple_apres\": \"return heavy_calc()\",\n",
        "    \"gain_cpu\": \"-5%\",\n",
        "    \"gain_ram\": \"-5%\",\n",
        "    \"outil_mesure\": [\"memory_profiler\", \"timeit\"],\n",
        "    \"estimation_co2\": \"≈0.02g CO₂ économisés pour 1M appels\"\n",
        "  },\n",
        "  {\n",
        "    \"id\": \"rule_008\",\n",
        "    \"langage\": \"Python\",\n",
        "    \"contexte\": \"Sérialisation JSON\",\n",
        "    \"anti_pattern\": \"Parsing JSON lent avec le module standard pour de gros fichiers\",\n",
        "    \"advice\": \"Utiliser orjson/ujson pour charger rapidement de gros JSON.\",\n",
        "    \"exemple_avant\": \"import json\\nobj = [json.loads(s) for s in big_strings]\",\n",
        "    \"exemple_apres\": \"import orjson\\nobj = [orjson.loads(s) for s in big_strings]\",\n",
        "    \"gain_cpu\": \"-50%\",\n",
        "    \"gain_ram\": \"-20%\",\n",
        "    \"outil_mesure\": [\"memory_profiler\", \"timeit\"],\n",
        "    \"estimation_co2\": \"≈0.20g CO₂ économisés pour 1M objets\"\n",
        "  },\n",
        "  {\n",
        "    \"id\": \"rule_009\",\n",
        "    \"langage\": \"Python\",\n",
        "    \"contexte\": \"Bases de données\",\n",
        "    \"anti_pattern\": \"Requêtes SQL non groupées dans une boucle\",\n",
        "    \"advice\": \"Utiliser executemany/bulk insert pour grouper les opérations.\",\n",
        "    \"exemple_avant\": \"for row in rows:\\n    cur.execute(\\\"INSERT INTO t VALUES (%s,%s)\\\", row)\",\n",
        "    \"exemple_apres\": \"cur.executemany(\\\"INSERT INTO t VALUES (%s,%s)\\\", rows)\",\n",
        "    \"gain_cpu\": \"-40%\",\n",
        "    \"gain_ram\": \"-25%\",\n",
        "    \"outil_mesure\": [\"memory_profiler\", \"timeit\"],\n",
        "    \"estimation_co2\": \"≈0.18g CO₂ économisés pour 1M lignes\"\n",
        "  },\n",
        "  {\n",
        "    \"id\": \"rule_010\",\n",
        "    \"langage\": \"Python\",\n",
        "    \"contexte\": \"Qualité de code\",\n",
        "    \"anti_pattern\": \"Imports non utilisés\",\n",
        "    \"advice\": \"Supprimer les imports inutiles pour réduire la mémoire et le temps de démarrage.\",\n",
        "    \"exemple_avant\": \"import numpy as np\\nimport pandas as pd\\nfrom math import sqrt\\nprint(\\\"ok\\\")\",\n",
        "    \"exemple_apres\": \"print(\\\"ok\\\")\",\n",
        "    \"gain_cpu\": \"-5%\",\n",
        "    \"gain_ram\": \"-10%\",\n",
        "    \"outil_mesure\": [\"memory_profiler\", \"timeit\"],\n",
        "    \"estimation_co2\": \"≈0.03g CO₂ économisés par exécution\"\n",
        "  },\n",
        "  {\n",
        "    \"id\": \"rule_011\",\n",
        "    \"langage\": \"Python\",\n",
        "    \"contexte\": \"Journalisation\",\n",
        "    \"anti_pattern\": \"Concaténation de chaînes dans les logs\",\n",
        "    \"advice\": \"Utiliser le formattage paresseux avec %s ou les paramètres du logger.\",\n",
        "    \"exemple_avant\": \"logger.debug(\\\"value=\\\" + str(expensive()))\",\n",
        "    \"exemple_apres\": \"logger.debug(\\\"value=%s\\\", expensive())\",\n",
        "    \"gain_cpu\": \"-25%\",\n",
        "    \"gain_ram\": \"-5%\",\n",
        "    \"outil_mesure\": [\"memory_profiler\", \"timeit\"],\n",
        "    \"estimation_co2\": \"≈0.10g CO₂ économisés pour 1M logs\"\n",
        "  },\n",
        "  {\n",
        "    \"id\": \"rule_012\",\n",
        "    \"langage\": \"Python\",\n",
        "    \"contexte\": \"Optimisation d'appels\",\n",
        "    \"anti_pattern\": \"Appels répétés de fonctions coûteuses avec mêmes entrées\",\n",
        "    \"advice\": \"Mettre en cache avec functools.lru_cache.\",\n",
        "    \"exemple_avant\": \"def fib(n):\\n    return n if n<2 else fib(n-1)+fib(n-2)\",\n",
        "    \"exemple_apres\": \"from functools import lru_cache\\n@lru_cache(None)\\ndef fib(n):\\n    return n if n<2 else fib(n-1)+fib(n-2)\",\n",
        "    \"gain_cpu\": \"-80%\",\n",
        "    \"gain_ram\": \"-20%\",\n",
        "    \"outil_mesure\": [\"memory_profiler\", \"timeit\"],\n",
        "    \"estimation_co2\": \"≈0.30g CO₂ économisés pour 1M appels\"\n",
        "  },\n",
        "  {\n",
        "    \"id\": \"rule_013\",\n",
        "    \"langage\": \"Python\",\n",
        "    \"contexte\": \"Gestion de mémoire\",\n",
        "    \"anti_pattern\": \"Sur-utilisation de deepcopy()\",\n",
        "    \"advice\": \"Utiliser des copies superficielles lorsque c'est sûr.\",\n",
        "    \"exemple_avant\": \"from copy import deepcopy\\nb = deepcopy(a)\",\n",
        "    \"exemple_apres\": \"from copy import copy\\nb = copy(a)  # ou utiliser une vue\",\n",
        "    \"gain_cpu\": \"-30%\",\n",
        "    \"gain_ram\": \"-45%\",\n",
        "    \"outil_mesure\": [\"memory_profiler\", \"timeit\"],\n",
        "    \"estimation_co2\": \"≈0.18g CO₂ économisés pour 1M objets\"\n",
        "  },\n",
        "  {\n",
        "    \"id\": \"rule_014\",\n",
        "    \"langage\": \"Python\",\n",
        "    \"contexte\": \"Algorithmes et boucles\",\n",
        "    \"anti_pattern\": \"Appel répété à len(list) dans une boucle\",\n",
        "    \"advice\": \"Mémoriser la longueur avant la boucle.\",\n",
        "    \"exemple_avant\": \"for i in range(len(lst)):\\n    do(lst[i])\",\n",
        "    \"exemple_apres\": \"n = len(lst)\\nfor i in range(n):\\n    do(lst[i])\",\n",
        "    \"gain_cpu\": \"-15%\",\n",
        "    \"gain_ram\": \"-0%\",\n",
        "    \"outil_mesure\": [\"memory_profiler\", \"timeit\"],\n",
        "    \"estimation_co2\": \"≈0.06g CO₂ économisés pour 1M itérations\"\n",
        "  },\n",
        "  {\n",
        "    \"id\": \"rule_015\",\n",
        "    \"langage\": \"Python\",\n",
        "    \"contexte\": \"CSV et parsing\",\n",
        "    \"anti_pattern\": \"Lecture CSV manuelle avec split() sans structure\",\n",
        "    \"advice\": \"Utiliser csv.DictReader ou pandas pour un parsing structuré.\",\n",
        "    \"exemple_avant\": \"for line in open(path):\\n    cols = line.strip().split(',')\\n    use(cols)\",\n",
        "    \"exemple_apres\": \"import csv\\nwith open(path) as f:\\n    for row in csv.DictReader(f):\\n        use(row)\",\n",
        "    \"gain_cpu\": \"-25%\",\n",
        "    \"gain_ram\": \"-25%\",\n",
        "    \"outil_mesure\": [\"memory_profiler\", \"timeit\"],\n",
        "    \"estimation_co2\": \"≈0.12g CO₂ économisés pour 1M lignes\"\n",
        "  },\n",
        "  {\n",
        "    \"id\": \"rule_016\",\n",
        "    \"langage\": \"Python\",\n",
        "    \"contexte\": \"Itérateurs\",\n",
        "    \"anti_pattern\": \"Découpage (slicing) répété de grandes listes\",\n",
        "    \"advice\": \"Utiliser des itérateurs ou itertools.islice pour éviter des copies.\",\n",
        "    \"exemple_avant\": \"for i in range(0, len(lst), 1000):\\n    chunk = lst[i:i+1000]\\n    process(chunk)\",\n",
        "    \"exemple_apres\": \"from itertools import islice\\nit = iter(lst)\\nwhile True:\\n    chunk = list(islice(it, 1000))\\n    if not chunk: break\\n    process(chunk)\",\n",
        "    \"gain_cpu\": \"-25%\",\n",
        "    \"gain_ram\": \"-40%\",\n",
        "    \"outil_mesure\": [\"memory_profiler\", \"timeit\"],\n",
        "    \"estimation_co2\": \"≈0.16g CO₂ économisés pour 1M éléments\"\n",
        "  },\n",
        "  {\n",
        "    \"id\": \"rule_017\",\n",
        "    \"langage\": \"Python\",\n",
        "    \"contexte\": \"Gestion d'exceptions\",\n",
        "    \"anti_pattern\": \"Utiliser les exceptions pour le contrôle de flux\",\n",
        "    \"advice\": \"Tester les conditions (in, get, try léger) plutôt que lever/capturer souvent.\",\n",
        "    \"exemple_avant\": \"for k in keys:\\n    try:\\n        total += d[k]\\n    except KeyError:\\n        pass\",\n",
        "    \"exemple_apres\": \"for k in keys:\\n    total += d.get(k, 0)\",\n",
        "    \"gain_cpu\": \"-50%\",\n",
        "    \"gain_ram\": \"-5%\",\n",
        "    \"outil_mesure\": [\"memory_profiler\", \"timeit\"],\n",
        "    \"estimation_co2\": \"≈0.18g CO₂ économisés pour 1M itérations\"\n",
        "  },\n",
        "  {\n",
        "    \"id\": \"rule_018\",\n",
        "    \"langage\": \"Python\",\n",
        "    \"contexte\": \"Concurrence et ressources\",\n",
        "    \"anti_pattern\": \"Nettoyage manuel des ressources (pools, sockets) sans contexte\",\n",
        "    \"advice\": \"Utiliser des gestionnaires de contexte pour garantir le cleanup.\",\n",
        "    \"exemple_avant\": \"from multiprocessing import Pool\\npool = Pool()\\nres = pool.map(f, items)\\npool.close(); pool.join()\",\n",
        "    \"exemple_apres\": \"from multiprocessing import Pool\\nwith Pool() as pool:\\n    res = pool.map(f, items)\",\n",
        "    \"gain_cpu\": \"-10%\",\n",
        "    \"gain_ram\": \"-20%\",\n",
        "    \"outil_mesure\": [\"memory_profiler\", \"timeit\"],\n",
        "    \"estimation_co2\": \"≈0.08g CO₂ économisés pour 1M tâches\"\n",
        "  },\n",
        "  {\n",
        "    \"id\": \"rule_019\",\n",
        "    \"langage\": \"Python\",\n",
        "    \"contexte\": \"NumPy\",\n",
        "    \"anti_pattern\": \"Boucles Python sur des tableaux NumPy\",\n",
        "    \"advice\": \"Vectoriser les opérations NumPy.\",\n",
        "    \"exemple_avant\": \"out = np.empty_like(a)\\nfor i in range(len(a)):\\n    out[i] = a[i] * 2\",\n",
        "    \"exemple_apres\": \"out = a * 2\",\n",
        "    \"gain_cpu\": \"-80%\",\n",
        "    \"gain_ram\": \"-20%\",\n",
        "    \"outil_mesure\": [\"memory_profiler\", \"timeit\"],\n",
        "    \"estimation_co2\": \"≈0.30g CO₂ économisés pour 1M éléments\"\n",
        "  },\n",
        "  {\n",
        "    \"id\": \"rule_020\",\n",
        "    \"langage\": \"Python\",\n",
        "    \"contexte\": \"pandas\",\n",
        "    \"anti_pattern\": \"DataFrame.apply() ligne par ligne\",\n",
        "    \"advice\": \"Utiliser des opérations vectorisées pandas.\",\n",
        "    \"exemple_avant\": \"df[\\\"y\\\"] = df[\\\"x\\\"].apply(lambda v: v*2)\",\n",
        "    \"exemple_apres\": \"df[\\\"y\\\"] = df[\\\"x\\\"] * 2\",\n",
        "    \"gain_cpu\": \"-70%\",\n",
        "    \"gain_ram\": \"-20%\",\n",
        "    \"outil_mesure\": [\"memory_profiler\", \"timeit\"],\n",
        "    \"estimation_co2\": \"≈0.26g CO₂ économisés pour 1M lignes\"\n",
        "  },\n",
        "  {\n",
        "    \"id\": \"rule_021\",\n",
        "    \"langage\": \"Python\",\n",
        "    \"contexte\": \"pandas\",\n",
        "    \"anti_pattern\": \"Itération ligne par ligne sur un DataFrame\",\n",
        "    \"advice\": \"Préférer les opérations vectorisées ou itertuples() si nécessaire.\",\n",
        "    \"exemple_avant\": \"for _, row in df.iterrows():\\n    total += row[\\\"x\\\"]\",\n",
        "    \"exemple_apres\": \"total = df[\\\"x\\\"].sum()  # ou for r in df.itertuples(): total += r.x\",\n",
        "    \"gain_cpu\": \"-65%\",\n",
        "    \"gain_ram\": \"-15%\",\n",
        "    \"outil_mesure\": [\"memory_profiler\", \"timeit\"],\n",
        "    \"estimation_co2\": \"≈0.24g CO₂ économisés pour 1M lignes\"\n",
        "  },\n",
        "  {\n",
        "    \"id\": \"rule_022\",\n",
        "    \"langage\": \"Python\",\n",
        "    \"contexte\": \"Structures de données\",\n",
        "    \"anti_pattern\": \"Boucles imbriquées avec tests d'appartenance sur des listes\",\n",
        "    \"advice\": \"Remplacer les listes par des sets/dicts pour les membership tests.\",\n",
        "    \"exemple_avant\": \"count = 0\\nfor x in A:\\n    if x in B:\\n        count += 1\",\n",
        "    \"exemple_apres\": \"B_set = set(B)\\ncount = sum(1 for x in A if x in B_set)\",\n",
        "    \"gain_cpu\": \"-70%\",\n",
        "    \"gain_ram\": \"-20%\",\n",
        "    \"outil_mesure\": [\"memory_profiler\", \"timeit\"],\n",
        "    \"estimation_co2\": \"≈0.26g CO₂ économisés pour 1M comparaisons\"\n",
        "  },\n",
        "  {\n",
        "    \"id\": \"rule_023\",\n",
        "    \"langage\": \"Python\",\n",
        "    \"contexte\": \"Chaînes de caractères\",\n",
        "    \"anti_pattern\": \"Concaténation lente pour formatage\",\n",
        "    \"advice\": \"Utiliser des f-strings ou str.format pour lisibilité et performance.\",\n",
        "    \"exemple_avant\": \"msg = \\\"Hello \\\" + name + \\\", id=\\\" + str(i)\",\n",
        "    \"exemple_apres\": \"msg = f\\\"Hello {name}, id={i}\\\"\",\n",
        "    \"gain_cpu\": \"-20%\",\n",
        "    \"gain_ram\": \"-5%\",\n",
        "    \"outil_mesure\": [\"memory_profiler\", \"timeit\"],\n",
        "    \"estimation_co2\": \"≈0.08g CO₂ économisés pour 1M chaînes\"\n",
        "  },\n",
        "  {\n",
        "    \"id\": \"rule_024\",\n",
        "    \"langage\": \"Python\",\n",
        "    \"contexte\": \"Algorithmes et boucles\",\n",
        "    \"anti_pattern\": \"Croissance inefficace d'une liste par append dans de très grandes boucles\",\n",
        "    \"advice\": \"Utiliser des compréhensions ou pré-allouer si possible.\",\n",
        "    \"exemple_avant\": \"res = []\\nfor x in data:\\n    res.append(f(x))\",\n",
        "    \"exemple_apres\": \"res = [f(x) for x in data]\",\n",
        "    \"gain_cpu\": \"-25%\",\n",
        "    \"gain_ram\": \"-20%\",\n",
        "    \"outil_mesure\": [\"memory_profiler\", \"timeit\"],\n",
        "    \"estimation_co2\": \"≈0.12g CO₂ économisés pour 1M éléments\"\n",
        "  },\n",
        "  {\n",
        "    \"id\": \"rule_025\",\n",
        "    \"langage\": \"Python\",\n",
        "    \"contexte\": \"Sérialisation\",\n",
        "    \"anti_pattern\": \"Utilisation inefficace de pickle pour de gros objets\",\n",
        "    \"advice\": \"Utiliser joblib/orjson/rapids pour objets volumineux ou NumPy.\",\n",
        "    \"exemple_avant\": \"import pickle\\npickle.dump(obj, open(\\\"f.pkl\\\",\\\"wb\\\"))\",\n",
        "    \"exemple_apres\": \"from joblib import dump\\ndump(obj, \\\"f.joblib\\\")\",\n",
        "    \"gain_cpu\": \"-30%\",\n",
        "    \"gain_ram\": \"-25%\",\n",
        "    \"outil_mesure\": [\"memory_profiler\", \"timeit\"],\n",
        "    \"estimation_co2\": \"≈0.14g CO₂ économisés pour 1M enregistrements\"\n",
        "  },\n",
        "  {\n",
        "    \"id\": \"rule_026\",\n",
        "    \"langage\": \"Python\",\n",
        "    \"contexte\": \"Qualité de code\",\n",
        "    \"anti_pattern\": \"Variables globales persistantes\",\n",
        "    \"advice\": \"Éviter les globales, passer les données en paramètres ou via objets.\",\n",
        "    \"exemple_avant\": \"cache = []\\ndef add(x):\\n    global cache\\n    cache.append(x)\",\n",
        "    \"exemple_apres\": \"def add(cache, x):\\n    cache.append(x)\",\n",
        "    \"gain_cpu\": \"-5%\",\n",
        "    \"gain_ram\": \"-20%\",\n",
        "    \"outil_mesure\": [\"memory_profiler\", \"timeit\"],\n",
        "    \"estimation_co2\": \"≈0.06g CO₂ économisés pour 1M appels\"\n",
        "  },\n",
        "  {\n",
        "    \"id\": \"rule_027\",\n",
        "    \"langage\": \"Python\",\n",
        "    \"contexte\": \"Chaînes de caractères\",\n",
        "    \"anti_pattern\": \"Découpage de chaîne manuel avec tranches/regex\",\n",
        "    \"advice\": \"Utiliser str.split quand c'est possible.\",\n",
        "    \"exemple_avant\": \"area = text[:3]; number = text[4:]\",\n",
        "    \"exemple_apres\": \"area, number = text.split('-')\",\n",
        "    \"gain_cpu\": \"-15%\",\n",
        "    \"gain_ram\": \"-5%\",\n",
        "    \"outil_mesure\": [\"memory_profiler\", \"timeit\"],\n",
        "    \"estimation_co2\": \"≈0.06g CO₂ économisés pour 1M découpages\"\n",
        "  },\n",
        "  {\n",
        "    \"id\": \"rule_028\",\n",
        "    \"langage\": \"Python\",\n",
        "    \"contexte\": \"pandas\",\n",
        "    \"anti_pattern\": \"Concaténation répétée de DataFrames dans une boucle\",\n",
        "    \"advice\": \"Accumuler dans une liste puis utiliser pd.concat une seule fois.\",\n",
        "    \"exemple_avant\": \"df = pd.DataFrame()\\nfor part in parts:\\n    df = pd.concat([df, part])\",\n",
        "    \"exemple_apres\": \"df = pd.concat(parts, ignore_index=True)\",\n",
        "    \"gain_cpu\": \"-65%\",\n",
        "    \"gain_ram\": \"-60%\",\n",
        "    \"outil_mesure\": [\"memory_profiler\", \"timeit\"],\n",
        "    \"estimation_co2\": \"≈0.28g CO₂ économisés pour 1M lignes\"\n",
        "  },\n",
        "  {\n",
        "    \"id\": \"rule_029\",\n",
        "    \"langage\": \"Python\",\n",
        "    \"contexte\": \"CSV et I/O\",\n",
        "    \"anti_pattern\": \"Écriture CSV non tamponnée, ligne par ligne\",\n",
        "    \"advice\": \"Utiliser un buffer ou pandas.to_csv avec chunksize.\",\n",
        "    \"exemple_avant\": \"with open(path, 'w', newline='') as f:\\n    w = csv.writer(f)\\n    for r in rows:\\n        w.writerow(r)\",\n",
        "    \"exemple_apres\": \"with open(path, 'w', newline='') as f:\\n    w = csv.writer(f)\\n    w.writerows(rows)  # ou pandas.to_csv(chunksize=...)\\n\",\n",
        "    \"gain_cpu\": \"-20%\",\n",
        "    \"gain_ram\": \"-15%\",\n",
        "    \"outil_mesure\": [\"memory_profiler\", \"timeit\"],\n",
        "    \"estimation_co2\": \"≈0.10g CO₂ économisés pour 1M lignes\"\n",
        "  },\n",
        "  {\n",
        "    \"id\": \"rule_030\",\n",
        "    \"langage\": \"Python\",\n",
        "    \"contexte\": \"pandas et mémoire\",\n",
        "    \"anti_pattern\": \"Copies de DataFrame non nécessaires via .copy()\",\n",
        "    \"advice\": \"Éviter .copy() inutile, préférer les vues ou opérations en place lorsque c'est sûr.\",\n",
        "    \"exemple_avant\": \"df2 = df.copy()\\ndf2[\\\"x\\\"] *= 2\",\n",
        "    \"exemple_apres\": \"df[\\\"x\\\"] *= 2  # si l'aliasing est accepté\",\n",
        "    \"gain_cpu\": \"-20%\",\n",
        "    \"gain_ram\": \"-50%\",\n",
        "    \"outil_mesure\": [\"memory_profiler\", \"timeit\"],\n",
        "    \"estimation_co2\": \"≈0.20g CO₂ économisés pour 1M lignes\"\n",
        "   },\n",
        "\n",
        "    {\n",
        "    \"id\": \"rule_031\",\n",
        "    \"langage\": \"Python\",\n",
        "    \"contexte\": \"Algorithmes et listes\",\n",
        "    \"anti_pattern\": \"Aplatissement manuel de listes imbriquées avec des boucles\",\n",
        "    \"advice\": \"Utiliser itertools.chain.from_iterable pour aplatir efficacement.\",\n",
        "    \"exemple_avant\": \"flat = []\\nfor sub in lists:\\n    for x in sub:\\n        flat.append(x)\",\n",
        "    \"exemple_apres\": \"from itertools import chain\\nflat = list(chain.from_iterable(lists))\",\n",
        "    \"gain_cpu\": \"-30%\",\n",
        "    \"gain_ram\": \"-10%\",\n",
        "    \"outil_mesure\": [\"memory_profiler\", \"timeit\"],\n",
        "    \"estimation_co2\": \"≈0.12g CO₂ économisés pour 1M éléments\"\n",
        "  },\n",
        "  {\n",
        "    \"id\": \"rule_032\",\n",
        "    \"langage\": \"Python\",\n",
        "    \"contexte\": \"Algèbre linéaire\",\n",
        "    \"anti_pattern\": \"Multiplication de matrices avec des boucles Python\",\n",
        "    \"advice\": \"Utiliser NumPy (BLAS optimisé) ou l’opérateur @.\",\n",
        "    \"exemple_avant\": \"C = [[sum(a*b for a,b in zip(row,col)) for col in zip(*B)] for row in A]\",\n",
        "    \"exemple_apres\": \"import numpy as np\\nC = A @ B  # A et B sont des tableaux NumPy\",\n",
        "    \"gain_cpu\": \"-85%\",\n",
        "    \"gain_ram\": \"-20%\",\n",
        "    \"outil_mesure\": [\"memory_profiler\", \"timeit\"],\n",
        "    \"estimation_co2\": \"≈0.34g CO₂ économisés pour 1M produits scalaires\"\n",
        "  },\n",
        "  {\n",
        "    \"id\": \"rule_033\",\n",
        "    \"langage\": \"Python\",\n",
        "    \"contexte\": \"Fichiers et I/O\",\n",
        "    \"anti_pattern\": \"readlines() sur de gros fichiers avant itération\",\n",
        "    \"advice\": \"Itérer directement sur l’objet fichier (streaming).\",\n",
        "    \"exemple_avant\": \"lines = open(path).readlines()\\nfor line in lines:\\n    process(line)\",\n",
        "    \"exemple_apres\": \"with open(path) as f:\\n    for line in f:\\n        process(line)\",\n",
        "    \"gain_cpu\": \"-10%\",\n",
        "    \"gain_ram\": \"-40%\",\n",
        "    \"outil_mesure\": [\"memory_profiler\", \"timeit\"],\n",
        "    \"estimation_co2\": \"≈0.10g CO₂ économisés pour 1M lignes\"\n",
        "  },\n",
        "  {\n",
        "    \"id\": \"rule_034\",\n",
        "    \"langage\": \"Python\",\n",
        "    \"contexte\": \"Structures de données\",\n",
        "    \"anti_pattern\": \"Initialisation manuelle d’un compteur/dict avec tests\",\n",
        "    \"advice\": \"Utiliser collections.Counter ou defaultdict.\",\n",
        "    \"exemple_avant\": \"counts = {}\\nfor k in keys:\\n    if k in counts:\\n        counts[k] += 1\\n    else:\\n        counts[k] = 1\",\n",
        "    \"exemple_apres\": \"from collections import Counter\\ncounts = Counter(keys)\",\n",
        "    \"gain_cpu\": \"-40%\",\n",
        "    \"gain_ram\": \"-10%\",\n",
        "    \"outil_mesure\": [\"memory_profiler\", \"timeit\"],\n",
        "    \"estimation_co2\": \"≈0.16g CO₂ économisés pour 1M éléments\"\n",
        "  },\n",
        "  {\n",
        "    \"id\": \"rule_035\",\n",
        "    \"langage\": \"Python\",\n",
        "    \"contexte\": \"Optimisation de boucles\",\n",
        "    \"anti_pattern\": \"Conversions de type répétées dans une boucle\",\n",
        "    \"advice\": \"Convertir une fois en amont ou vectoriser.\",\n",
        "    \"exemple_avant\": \"total = 0\\nfor x in values:\\n    total += int(x)\",\n",
        "    \"exemple_apres\": \"values = list(map(int, values))\\ntotal = sum(values)\",\n",
        "    \"gain_cpu\": \"-20%\",\n",
        "    \"gain_ram\": \"-5%\",\n",
        "    \"outil_mesure\": [\"memory_profiler\", \"timeit\"],\n",
        "    \"estimation_co2\": \"≈0.08g CO₂ économisés pour 1M conversions\"\n",
        "  },\n",
        "  {\n",
        "    \"id\": \"rule_036\",\n",
        "    \"langage\": \"Python\",\n",
        "    \"contexte\": \"Dates et parsing\",\n",
        "    \"anti_pattern\": \"datetime.strptime dans une boucle sur de gros volumes\",\n",
        "    \"advice\": \"Utiliser pandas.to_datetime ou ciso8601 pour vectoriser/accélérer.\",\n",
        "    \"exemple_avant\": \"from datetime import datetime\\nfor s in dates:\\n    d = datetime.strptime(s, '%Y-%m-%d')\\n    use(d)\",\n",
        "    \"exemple_apres\": \"import pandas as pd\\nfor d in pd.to_datetime(dates, format='%Y-%m-%d'):\\n    use(d)\",\n",
        "    \"gain_cpu\": \"-60%\",\n",
        "    \"gain_ram\": \"-10%\",\n",
        "    \"outil_mesure\": [\"memory_profiler\", \"timeit\"],\n",
        "    \"estimation_co2\": \"≈0.24g CO₂ économisés pour 1M dates\"\n",
        "  },\n",
        "  {\n",
        "    \"id\": \"rule_037\",\n",
        "    \"langage\": \"Python\",\n",
        "    \"contexte\": \"Concurrence (threads)\",\n",
        "    \"anti_pattern\": \"Gestion manuelle d’un pool de threads\",\n",
        "    \"advice\": \"Utiliser ThreadPoolExecutor pour mapper les tâches.\",\n",
        "    \"exemple_avant\": \"from threading import Thread\\nthreads = []\\nfor x in tasks:\\n    t = Thread(target=work, args=(x,))\\n    t.start(); threads.append(t)\\nfor t in threads:\\n    t.join()\",\n",
        "    \"exemple_apres\": \"from concurrent.futures import ThreadPoolExecutor\\nwith ThreadPoolExecutor() as ex:\\n    list(ex.map(work, tasks))\",\n",
        "    \"gain_cpu\": \"-25%\",\n",
        "    \"gain_ram\": \"-10%\",\n",
        "    \"outil_mesure\": [\"memory_profiler\", \"timeit\"],\n",
        "    \"estimation_co2\": \"≈0.10g CO₂ économisés pour 1M tâches\"\n",
        "  },\n",
        "  {\n",
        "    \"id\": \"rule_038\",\n",
        "    \"langage\": \"Python\",\n",
        "    \"contexte\": \"Concurrence (processus)\",\n",
        "    \"anti_pattern\": \"Création de très nombreux processus non réutilisés\",\n",
        "    \"advice\": \"Réutiliser un Pool de processus.\",\n",
        "    \"exemple_avant\": \"from multiprocessing import Process\\nfor chunk in chunks:\\n    p = Process(target=work, args=(chunk,))\\n    p.start(); p.join()\",\n",
        "    \"exemple_apres\": \"from multiprocessing import Pool\\nwith Pool() as pool:\\n    pool.map(work, chunks)\",\n",
        "    \"gain_cpu\": \"-35%\",\n",
        "    \"gain_ram\": \"-30%\",\n",
        "    \"outil_mesure\": [\"memory_profiler\", \"timeit\"],\n",
        "    \"estimation_co2\": \"≈0.14g CO₂ économisés pour 1M tâches\"\n",
        "  },\n",
        "  {\n",
        "    \"id\": \"rule_039\",\n",
        "    \"langage\": \"Python\",\n",
        "    \"contexte\": \"Qualité de code\",\n",
        "    \"anti_pattern\": \"Embranchements et tests redondants imbriqués\",\n",
        "    \"advice\": \"Aplatir et simplifier les conditions.\",\n",
        "    \"exemple_avant\": \"if a:\\n    if b:\\n        if not c:\\n            do()\",\n",
        "    \"exemple_apres\": \"if a and b and not c:\\n    do()\",\n",
        "    \"gain_cpu\": \"-10%\",\n",
        "    \"gain_ram\": \"-0%\",\n",
        "    \"outil_mesure\": [\"memory_profiler\", \"timeit\"],\n",
        "    \"estimation_co2\": \"≈0.04g CO₂ économisés pour 1M tests\"\n",
        "  },\n",
        "  {\n",
        "    \"id\": \"rule_040\",\n",
        "    \"langage\": \"Python\",\n",
        "    \"contexte\": \"Aléatoire et vecteur\",\n",
        "    \"anti_pattern\": \"Génération de nombres aléatoires un par un avec random.random\",\n",
        "    \"advice\": \"Utiliser numpy.random pour une génération vectorisée.\",\n",
        "    \"exemple_avant\": \"import random\\nvals = [random.random() for _ in range(n)]\",\n",
        "    \"exemple_apres\": \"import numpy as np\\nvals = np.random.random(n)\",\n",
        "    \"gain_cpu\": \"-70%\",\n",
        "    \"gain_ram\": \"-10%\",\n",
        "    \"outil_mesure\": [\"memory_profiler\", \"timeit\"],\n",
        "    \"estimation_co2\": \"≈0.28g CO₂ économisés pour 1M tirages\"\n",
        "  },\n",
        "  {\n",
        "    \"id\": \"rule_041\",\n",
        "    \"langage\": \"Python\",\n",
        "    \"contexte\": \"XML (streaming)\",\n",
        "    \"anti_pattern\": \"Chargement complet de l’arbre XML avec parse()\",\n",
        "    \"advice\": \"Utiliser iterparse et nettoyer les nœuds au fil de l’eau.\",\n",
        "    \"exemple_avant\": \"import xml.etree.ElementTree as ET\\nroot = ET.parse(file).getroot()\\nfor elem in root.iter('item'):\\n    process(elem)\",\n",
        "    \"exemple_apres\": \"import xml.etree.ElementTree as ET\\nfor event, elem in ET.iterparse(file, events=('end',)):\\n    if elem.tag == 'item':\\n        process(elem)\\n        elem.clear()\",\n",
        "    \"gain_cpu\": \"-30%\",\n",
        "    \"gain_ram\": \"-50%\",\n",
        "    \"outil_mesure\": [\"memory_profiler\", \"timeit\"],\n",
        "    \"estimation_co2\": \"≈0.18g CO₂ économisés pour 1M éléments\"\n",
        "  },\n",
        "  {\n",
        "    \"id\": \"rule_042\",\n",
        "    \"langage\": \"Python\",\n",
        "    \"contexte\": \"Mémorisation des résultats\",\n",
        "    \"anti_pattern\": \"Recalcul d’un résultat coûteux pour les mêmes entrées\",\n",
        "    \"advice\": \"Utiliser functools.lru_cache pour mettre en cache.\",\n",
        "    \"exemple_avant\": \"def f(x):\\n    return expensive(x)\\nres = [f(x) for x in xs]\",\n",
        "    \"exemple_apres\": \"from functools import lru_cache\\n@lru_cache(None)\\ndef f(x):\\n    return expensive(x)\\nres = [f(x) for x in xs]\",\n",
        "    \"gain_cpu\": \"-60%\",\n",
        "    \"gain_ram\": \"-20%\",\n",
        "    \"outil_mesure\": [\"memory_profiler\", \"timeit\"],\n",
        "    \"estimation_co2\": \"≈0.24g CO₂ économisés pour 1M appels\"\n",
        "  },\n",
        "  {\n",
        "    \"id\": \"rule_043\",\n",
        "    \"langage\": \"Python\",\n",
        "    \"contexte\": \"Chaînes de caractères\",\n",
        "    \"anti_pattern\": \"Recherche simple via regex\",\n",
        "    \"advice\": \"Utiliser str.find/startswith/endswith pour les cas simples.\",\n",
        "    \"exemple_avant\": \"import re\\nif re.search(r'^abc', s):\\n    ...\",\n",
        "    \"exemple_apres\": \"if s.startswith('abc'):\\n    ...\",\n",
        "    \"gain_cpu\": \"-30%\",\n",
        "    \"gain_ram\": \"-5%\",\n",
        "    \"outil_mesure\": [\"memory_profiler\", \"timeit\"],\n",
        "    \"estimation_co2\": \"≈0.12g CO₂ économisés pour 1M tests\"\n",
        "  },\n",
        "  {\n",
        "    \"id\": \"rule_044\",\n",
        "    \"langage\": \"Python\",\n",
        "    \"contexte\": \"CSV volumineux / pandas\",\n",
        "    \"anti_pattern\": \"Lecture d’un gros CSV sans chunks (pic mémoire)\",\n",
        "    \"advice\": \"Lire par morceaux avec chunksize et traiter incrémentalement.\",\n",
        "    \"exemple_avant\": \"import pandas as pd\\ndf = pd.read_csv(path)\",\n",
        "    \"exemple_apres\": \"import pandas as pd\\nfor chunk in pd.read_csv(path, chunksize=100_000):\\n    process(chunk)\",\n",
        "    \"gain_cpu\": \"-20%\",\n",
        "    \"gain_ram\": \"-60%\",\n",
        "    \"outil_mesure\": [\"memory_profiler\", \"timeit\"],\n",
        "    \"estimation_co2\": \"≈0.16g CO₂ économisés pour 1M lignes\"\n",
        "  },\n",
        "  {\n",
        "    \"id\": \"rule_045\",\n",
        "    \"langage\": \"Python\",\n",
        "    \"contexte\": \"Calcul numérique\",\n",
        "    \"anti_pattern\": \"Boucles Python pour des calculs mathématiques élément par élément\",\n",
        "    \"advice\": \"Vectoriser avec NumPy.\",\n",
        "    \"exemple_avant\": \"out = []\\nfor x in a:\\n    out.append(math.sin(x) + x*x)\",\n",
        "    \"exemple_apres\": \"import numpy as np\\nout = np.sin(a) + a*a\",\n",
        "    \"gain_cpu\": \"-80%\",\n",
        "    \"gain_ram\": \"-20%\",\n",
        "    \"outil_mesure\": [\"memory_profiler\", \"timeit\"],\n",
        "    \"estimation_co2\": \"≈0.30g CO₂ économisés pour 1M éléments\"\n",
        "  },\n",
        "  {\n",
        "    \"id\": \"rule_046\",\n",
        "    \"langage\": \"Python\",\n",
        "    \"contexte\": \"Dictionnaires\",\n",
        "    \"anti_pattern\": \"Fusion de dictionnaires par boucles manuelles\",\n",
        "    \"advice\": \"Utiliser l’unpacking {**d1, **d2} ou ChainMap.\",\n",
        "    \"exemple_avant\": \"merged = {}\\nfor k, v in d1.items():\\n    merged[k] = v\\nfor k, v in d2.items():\\n    merged[k] = v\",\n",
        "    \"exemple_apres\": \"merged = {**d1, **d2}  # ou: from collections import ChainMap; merged = dict(ChainMap(d2, d1))\",\n",
        "    \"gain_cpu\": \"-15%\",\n",
        "    \"gain_ram\": \"-5%\",\n",
        "    \"outil_mesure\": [\"memory_profiler\", \"timeit\"],\n",
        "    \"estimation_co2\": \"≈0.06g CO₂ économisés pour 1M clés\"\n",
        "  },\n",
        "  {\n",
        "    \"id\": \"rule_047\",\n",
        "    \"langage\": \"Python\",\n",
        "    \"contexte\": \"Collections\",\n",
        "    \"anti_pattern\": \"Comptage manuel d’items dans une boucle\",\n",
        "    \"advice\": \"Utiliser collections.Counter.\",\n",
        "    \"exemple_avant\": \"counts = {}\\nfor x in data:\\n    counts[x] = counts.get(x, 0) + 1\",\n",
        "    \"exemple_apres\": \"from collections import Counter\\ncounts = Counter(data)\",\n",
        "    \"gain_cpu\": \"-30%\",\n",
        "    \"gain_ram\": \"-10%\",\n",
        "    \"outil_mesure\": [\"memory_profiler\", \"timeit\"],\n",
        "    \"estimation_co2\": \"≈0.12g CO₂ économisés pour 1M éléments\"\n",
        "  },\n",
        "  {\n",
        "    \"id\": \"rule_048\",\n",
        "    \"langage\": \"Python\",\n",
        "    \"contexte\": \"Aléatoire\",\n",
        "    \"anti_pattern\": \"Reseeding à chaque itération\",\n",
        "    \"advice\": \"Initialiser la graine une seule fois en amont.\",\n",
        "    \"exemple_avant\": \"import random\\nfor _ in range(n):\\n    random.seed(42)\\n    do()\",\n",
        "    \"exemple_apres\": \"import random\\nrandom.seed(42)\\nfor _ in range(n):\\n    do()\",\n",
        "    \"gain_cpu\": \"-15%\",\n",
        "    \"gain_ram\": \"-0%\",\n",
        "    \"outil_mesure\": [\"memory_profiler\", \"timeit\"],\n",
        "    \"estimation_co2\": \"≈0.04g CO₂ économisés pour 1M itérations\"\n",
        "  },\n",
        "  {\n",
        "    \"id\": \"rule_049\",\n",
        "    \"langage\": \"Python\",\n",
        "    \"contexte\": \"Qualité de code et mémoire\",\n",
        "    \"anti_pattern\": \"Arguments par défaut mutables (fuite mémoire/comportement inattendu)\",\n",
        "    \"advice\": \"Utiliser None et initialiser dans le corps.\",\n",
        "    \"exemple_avant\": \"def add(item, cache=[]):\\n    cache.append(item)\\n    return cache\",\n",
        "    \"exemple_apres\": \"def add(item, cache=None):\\n    if cache is None:\\n        cache = []\\n    cache.append(item)\\n    return cache\",\n",
        "    \"gain_cpu\": \"-0%\",\n",
        "    \"gain_ram\": \"-10%\",\n",
        "    \"outil_mesure\": [\"memory_profiler\", \"timeit\"],\n",
        "    \"estimation_co2\": \"Prévention d’anomalies; impact variable\"\n",
        "  },\n",
        "  {\n",
        "    \"id\": \"rule_050\",\n",
        "    \"langage\": \"Python\",\n",
        "    \"contexte\": \"Tri et clés\",\n",
        "    \"anti_pattern\": \"Utiliser une comparaison (cmp) coûteuse qui recalcule la clé\",\n",
        "    \"advice\": \"Pré-calculer des clés et utiliser le paramètre key du tri.\",\n",
        "    \"exemple_avant\": \"from functools import cmp_to_key\\nsorted(recs, key=cmp_to_key(lambda a,b: expensive(a)-expensive(b)))\",\n",
        "    \"exemple_apres\": \"keys = {r: expensive(r) for r in recs}\\nsorted(recs, key=lambda r: keys[r])\",\n",
        "    \"gain_cpu\": \"-50%\",\n",
        "    \"gain_ram\": \"-10%\",\n",
        "    \"outil_mesure\": [\"memory_profiler\", \"timeit\"],\n",
        "    \"estimation_co2\": \"≈0.20g CO₂ économisés pour 1M comparaisons\"\n",
        "  },\n",
        "  {\n",
        "    \"id\": \"rule_051\",\n",
        "    \"langage\": \"Python\",\n",
        "    \"contexte\": \"Lisibilité et mémoire\",\n",
        "    \"anti_pattern\": \"Compréhensions profondément imbriquées créant de gros intermédiaires\",\n",
        "    \"advice\": \"Décomposer ou utiliser un générateur pour être paresseux.\",\n",
        "    \"exemple_avant\": \"res = [[f(x,y) for y in ys] for x in xs]\",\n",
        "    \"exemple_apres\": \"res = (f(x, y) for x in xs for y in ys)  # générateur\",\n",
        "    \"gain_cpu\": \"-15%\",\n",
        "    \"gain_ram\": \"-35%\",\n",
        "    \"outil_mesure\": [\"memory_profiler\", \"timeit\"],\n",
        "    \"estimation_co2\": \"≈0.12g CO₂ économisés pour 1M éléments\"\n",
        "  },\n",
        "  {\n",
        "    \"id\": \"rule_052\",\n",
        "    \"langage\": \"Python\",\n",
        "    \"contexte\": \"Fichiers / OS\",\n",
        "    \"anti_pattern\": \"Appels os.path répétés dans une boucle sans mise en cache\",\n",
        "    \"advice\": \"Mettre en variable les fonctions utilisées fréquemment.\",\n",
        "    \"exemple_avant\": \"import os\\nfor p in paths:\\n    if os.path.exists(os.path.join(base, p)):\\n        handle(p)\",\n",
        "    \"exemple_apres\": \"import os\\njoin, exists = os.path.join, os.path.exists\\nfor p in paths:\\n    if exists(join(base, p)):\\n        handle(p)\",\n",
        "    \"gain_cpu\": \"-10%\",\n",
        "    \"gain_ram\": \"-0%\",\n",
        "    \"outil_mesure\": [\"memory_profiler\", \"timeit\"],\n",
        "    \"estimation_co2\": \"≈0.04g CO₂ économisés pour 1M chemins\"\n",
        "  },\n",
        "  {\n",
        "    \"id\": \"rule_053\",\n",
        "    \"langage\": \"Python\",\n",
        "    \"contexte\": \"Système / sous-processus\",\n",
        "    \"anti_pattern\": \"Appels subprocess répétés dans une boucle\",\n",
        "    \"advice\": \"Regrouper en un appel batch quand c’est possible.\",\n",
        "    \"exemple_avant\": \"import subprocess\\nfor f in files:\\n    subprocess.run(['cmd', f])\",\n",
        "    \"exemple_apres\": \"import subprocess\\nsubprocess.run(['cmd'] + files)\",\n",
        "    \"gain_cpu\": \"-40%\",\n",
        "    \"gain_ram\": \"-10%\",\n",
        "    \"outil_mesure\": [\"memory_profiler\", \"timeit\"],\n",
        "    \"estimation_co2\": \"≈0.22g CO₂ économisés pour 1M fichiers\"\n",
        "  },\n",
        "  {\n",
        "    \"id\": \"rule_054\",\n",
        "    \"langage\": \"Python\",\n",
        "    \"contexte\": \"I/O fichiers\",\n",
        "    \"anti_pattern\": \"Écritures non tamponnées sur de gros volumes\",\n",
        "    \"advice\": \"Utiliser io.BufferedWriter ou mmap pour très gros fichiers.\",\n",
        "    \"exemple_avant\": \"with open(path, 'wb', buffering=0) as f:\\n    for b in blocks:\\n        f.write(b)\",\n",
        "    \"exemple_apres\": \"from io import BufferedWriter\\nwith BufferedWriter(open(path, 'wb')) as f:\\n    for b in blocks:\\n        f.write(b)\",\n",
        "    \"gain_cpu\": \"-20%\",\n",
        "    \"gain_ram\": \"-10%\",\n",
        "    \"outil_mesure\": [\"memory_profiler\", \"timeit\"],\n",
        "    \"estimation_co2\": \"≈0.10g CO₂ économisés pour 1M blocs\"\n",
        "  },\n",
        "  {\n",
        "    \"id\": \"rule_055\",\n",
        "    \"langage\": \"Python\",\n",
        "    \"contexte\": \"pandas (jointure)\",\n",
        "    \"anti_pattern\": \"Jointures faites ligne par ligne\",\n",
        "    \"advice\": \"Utiliser DataFrame.merge pour joindre en vectorisé.\",\n",
        "    \"exemple_avant\": \"for _, r in df1.iterrows():\\n    df1.loc[_, 'val'] = df2[df2.id == r.id]['val'].values[0]\",\n",
        "    \"exemple_apres\": \"df = df1.merge(df2[['id','val']], on='id', how='left')\",\n",
        "    \"gain_cpu\": \"-75%\",\n",
        "    \"gain_ram\": \"-50%\",\n",
        "    \"outil_mesure\": [\"memory_profiler\", \"timeit\"],\n",
        "    \"estimation_co2\": \"≈0.32g CO₂ économisés pour 1M lignes\"\n",
        "  },\n",
        "  {\n",
        "    \"id\": \"rule_056\",\n",
        "    \"langage\": \"Python\",\n",
        "    \"contexte\": \"pandas (agrégation)\",\n",
        "    \"anti_pattern\": \"Agrégation manuelle au lieu de groupby vectorisé\",\n",
        "    \"advice\": \"Utiliser groupby + opérations vectorisées.\",\n",
        "    \"exemple_avant\": \"tot = {}\\nfor _, r in df.iterrows():\\n    tot[r['k']] = tot.get(r['k'], 0) + r['v']\",\n",
        "    \"exemple_apres\": \"res = df.groupby('k')['v'].sum().reset_index()\",\n",
        "    \"gain_cpu\": \"-70%\",\n",
        "    \"gain_ram\": \"-20%\",\n",
        "    \"outil_mesure\": [\"memory_profiler\", \"timeit\"],\n",
        "    \"estimation_co2\": \"≈0.26g CO₂ économisés pour 1M lignes\"\n",
        "  },\n",
        "  {\n",
        "    \"id\": \"rule_057\",\n",
        "    \"langage\": \"Python\",\n",
        "    \"contexte\": \"Qualité de code\",\n",
        "    \"anti_pattern\": \"Constantes en dur dupliquées\",\n",
        "    \"advice\": \"Centraliser les constantes dans des variables/config.\",\n",
        "    \"exemple_avant\": \"price = base * (1 + 0.2)\",\n",
        "    \"exemple_apres\": \"TAX = 0.2\\nprice = base * (1 + TAX)\",\n",
        "    \"gain_cpu\": \"-0%\",\n",
        "    \"gain_ram\": \"-5%\",\n",
        "    \"outil_mesure\": [\"memory_profiler\", \"timeit\"],\n",
        "    \"estimation_co2\": \"Impact faible mais améliore la maintenabilité\"\n",
        "  },\n",
        "  {\n",
        "    \"id\": \"rule_058\",\n",
        "    \"langage\": \"Python\",\n",
        "    \"contexte\": \"Journalisation\",\n",
        "    \"anti_pattern\": \"Journalisation excessive dans les boucles critiques\",\n",
        "    \"advice\": \"Réduire le niveau/volume ou utiliser des handlers asynchrones.\",\n",
        "    \"exemple_avant\": \"import logging\\nfor rec in recs:\\n    logging.info('rec=%s', rec)\",\n",
        "    \"exemple_apres\": \"import logging\\nlogger = logging.getLogger(__name__)\\nif logger.isEnabledFor(logging.INFO):\\n    logger.info('Processed %d records', len(recs))\",\n",
        "    \"gain_cpu\": \"-30%\",\n",
        "    \"gain_ram\": \"-10%\",\n",
        "    \"outil_mesure\": [\"memory_profiler\", \"timeit\"],\n",
        "    \"estimation_co2\": \"≈0.18g CO₂ économisés pour 1M logs\"\n",
        "  },\n",
        "  {\n",
        "    \"id\": \"rule_059\",\n",
        "    \"langage\": \"Python\",\n",
        "    \"contexte\": \"Gestion d’exceptions\",\n",
        "    \"anti_pattern\": \"Capture trop large couvrant de longues sections\",\n",
        "    \"advice\": \"Encadrer finement et cibler les exceptions pertinentes.\",\n",
        "    \"exemple_avant\": \"try:\\n    big_block()\\nexcept Exception:\\n    handle()\",\n",
        "    \"exemple_apres\": \"try:\\n    risky()\\nexcept SpecificError:\\n    handle()\",\n",
        "    \"gain_cpu\": \"-10%\",\n",
        "    \"gain_ram\": \"-0%\",\n",
        "    \"outil_mesure\": [\"memory_profiler\", \"timeit\"],\n",
        "    \"estimation_co2\": \"≈0.04g CO₂ économisés pour 1M passages\"\n",
        "  },\n",
        "  {\n",
        "    \"id\": \"rule_060\",\n",
        "    \"langage\": \"Python\",\n",
        "    \"contexte\": \"CSV et parsing\",\n",
        "    \"anti_pattern\": \"Parsing CSV manuel avec split au lieu d’un parseur\",\n",
        "    \"advice\": \"Utiliser csv.reader ou pandas pour un parsing robuste.\",\n",
        "    \"exemple_avant\": \"for line in open('f.csv'):\\n    cols = line.strip().split(',')\\n    use(cols)\",\n",
        "    \"exemple_apres\": \"import csv\\nwith open('f.csv') as f:\\n    for row in csv.reader(f):\\n        use(row)\",\n",
        "    \"gain_cpu\": \"-25%\",\n",
        "    \"gain_ram\": \"-20%\",\n",
        "    \"outil_mesure\": [\"memory_profiler\", \"timeit\"],\n",
        "    \"estimation_co2\": \"≈0.12g CO₂ économisés pour 1M lignes\"\n",
        "  },\n",
        "    {\n",
        "    \"id\": \"rule_061\",\n",
        "    \"langage\": \"Python\",\n",
        "    \"contexte\": \"Itérateurs et collections\",\n",
        "    \"anti_pattern\": \"Conversion inutile d’itérables en liste avec list() sans besoin\",\n",
        "    \"advice\": \"Conserver les générateurs/itérateurs tant qu’une liste matérielle n’est pas requise.\",\n",
        "    \"exemple_avant\": \"items = list(generator())\\nfor x in items:\\n    use(x)\",\n",
        "    \"exemple_apres\": \"for x in generator():\\n    use(x)\",\n",
        "    \"gain_cpu\": \"-15%\",\n",
        "    \"gain_ram\": \"-35%\",\n",
        "    \"outil_mesure\": [\"memory_profiler\", \"timeit\"],\n",
        "    \"estimation_co2\": \"≈0.10g CO₂ économisés pour 1M éléments\"\n",
        "  },\n",
        "  {\n",
        "    \"id\": \"rule_062\",\n",
        "    \"langage\": \"Python\",\n",
        "    \"contexte\": \"XML et sérialisation\",\n",
        "    \"anti_pattern\": \"Construction complète d’un arbre XML en mémoire avant écriture\",\n",
        "    \"advice\": \"Utiliser des écritures incrémentales/streaming (iterwrite) pour limiter l’empreinte mémoire.\",\n",
        "    \"exemple_avant\": \"from xml.etree.ElementTree import Element, SubElement, tostring\\nroot = Element('root')\\nfor r in rows:\\n    e = SubElement(root, 'row')\\n    e.text = r\\nopen('f.xml','wb').write(tostring(root))\",\n",
        "    \"exemple_apres\": \"from lxml.etree import xmlfile\\nwith xmlfile('f.xml', encoding='utf-8') as xf:\\n    with xf.element('root'):\\n        for r in rows:\\n            with xf.element('row'):\\n                xf.write(r)\",\n",
        "    \"gain_cpu\": \"-20%\",\n",
        "    \"gain_ram\": \"-60%\",\n",
        "    \"outil_mesure\": [\"memory_profiler\", \"timeit\"],\n",
        "    \"estimation_co2\": \"≈0.18g CO₂ économisés pour 1M éléments\"\n",
        "  },\n",
        "  {\n",
        "    \"id\": \"rule_063\",\n",
        "    \"langage\": \"Python\",\n",
        "    \"contexte\": \"I/O réseau\",\n",
        "    \"anti_pattern\": \"Appels d’API non groupés un par un dans une boucle\",\n",
        "    \"advice\": \"Regrouper les requêtes (batch), pagination ou bulk endpoints.\",\n",
        "    \"exemple_avant\": \"for rec in records:\\n    requests.post(url, json=rec)\",\n",
        "    \"exemple_apres\": \"batch = list(chunk(records, 100))\\nfor part in batch:\\n    requests.post(url+'/bulk', json=part)\",\n",
        "    \"gain_cpu\": \"-10%\",\n",
        "    \"gain_ram\": \"-10%\",\n",
        "    \"outil_mesure\": [\"memory_profiler\", \"timeit\"],\n",
        "    \"estimation_co2\": \"≈0.14g CO₂ économisés pour 1M requêtes\"\n",
        "  },\n",
        "  {\n",
        "    \"id\": \"rule_064\",\n",
        "    \"langage\": \"Python\",\n",
        "    \"contexte\": \"pandas et mémoire\",\n",
        "    \"anti_pattern\": \"deepcopy() de DataFrames sans nécessité\",\n",
        "    \"advice\": \"Éviter deepcopy, préférer les vues ou copies légères et opérations en place.\",\n",
        "    \"exemple_avant\": \"from copy import deepcopy\\ndf2 = deepcopy(df)\\ndf2['x'] = df2['x'] * 2\",\n",
        "    \"exemple_apres\": \"df['x'] = df['x'] * 2  # si acceptable\\n# ou df2 = df.copy(deep=False)\",\n",
        "    \"gain_cpu\": \"-20%\",\n",
        "    \"gain_ram\": \"-50%\",\n",
        "    \"outil_mesure\": [\"memory_profiler\", \"timeit\"],\n",
        "    \"estimation_co2\": \"≈0.22g CO₂ économisés pour 1M lignes\"\n",
        "  },\n",
        "  {\n",
        "    \"id\": \"rule_065\",\n",
        "    \"langage\": \"Python\",\n",
        "    \"contexte\": \"Dates et séries temporelles\",\n",
        "    \"anti_pattern\": \"Itération jour par jour en Python pour générer des dates\",\n",
        "    \"advice\": \"Utiliser pandas.date_range ou numpy.datetime64 pour vectoriser.\",\n",
        "    \"exemple_avant\": \"from datetime import date, timedelta\\ncur = start\\ndates = []\\nwhile cur <= end:\\n    dates.append(cur)\\n    cur += timedelta(days=1)\",\n",
        "    \"exemple_apres\": \"import pandas as pd\\ndates = pd.date_range(start, end, freq='D')\",\n",
        "    \"gain_cpu\": \"-60%\",\n",
        "    \"gain_ram\": \"-15%\",\n",
        "    \"outil_mesure\": [\"memory_profiler\", \"timeit\"],\n",
        "    \"estimation_co2\": \"≈0.20g CO₂ économisés pour 1M dates\"\n",
        "  },\n",
        "  {\n",
        "    \"id\": \"rule_066\",\n",
        "    \"langage\": \"Python\",\n",
        "    \"contexte\": \"Ensembles et opérations ensemblistes\",\n",
        "    \"anti_pattern\": \"Boucles manuelles pour union/intersection/différence\",\n",
        "    \"advice\": \"Utiliser les opérations de set (|, &, -).\",\n",
        "    \"exemple_avant\": \"common = []\\nfor x in A:\\n    if x in B:\\n        common.append(x)\",\n",
        "    \"exemple_apres\": \"common = list(set(A) & set(B))\",\n",
        "    \"gain_cpu\": \"-70%\",\n",
        "    \"gain_ram\": \"-20%\",\n",
        "    \"outil_mesure\": [\"memory_profiler\", \"timeit\"],\n",
        "    \"estimation_co2\": \"≈0.26g CO₂ économisés pour 1M comparaisons\"\n",
        "  },\n",
        "  {\n",
        "    \"id\": \"rule_067\",\n",
        "    \"langage\": \"Python\",\n",
        "    \"contexte\": \"Fichiers compressés\",\n",
        "    \"anti_pattern\": \"Compression/décompression gzip sans buffering\",\n",
        "    \"advice\": \"Utiliser gzip.open avec buffer et mode adapté.\",\n",
        "    \"exemple_avant\": \"import gzip\\nwith gzip.open('f.gz','wb', compresslevel=9) as f:\\n    for b in blocks:\\n        f.write(b)\",\n",
        "    \"exemple_apres\": \"import gzip\\nfrom io import BufferedWriter\\nwith gzip.open('f.gz','wb', compresslevel=6) as g:\\n    with BufferedWriter(g) as f:\\n        for b in blocks:\\n            f.write(b)\",\n",
        "    \"gain_cpu\": \"-25%\",\n",
        "    \"gain_ram\": \"-10%\",\n",
        "    \"outil_mesure\": [\"memory_profiler\", \"timeit\"],\n",
        "    \"estimation_co2\": \"≈0.12g CO₂ économisés pour 1M blocs\"\n",
        "  },\n",
        "  {\n",
        "    \"id\": \"rule_068\",\n",
        "    \"langage\": \"Python\",\n",
        "    \"contexte\": \"JSON volumineux\",\n",
        "    \"anti_pattern\": \"Chargement complet des gros JSON en mémoire\",\n",
        "    \"advice\": \"Utiliser ijson pour le streaming incrémental.\",\n",
        "    \"exemple_avant\": \"import json\\nobj = json.load(open('big.json'))\\nfor rec in obj['items']:\\n    process(rec)\",\n",
        "    \"exemple_apres\": \"import ijson\\nwith open('big.json','rb') as f:\\n    for rec in ijson.items(f, 'items.item'):\\n        process(rec)\",\n",
        "    \"gain_cpu\": \"-20%\",\n",
        "    \"gain_ram\": \"-70%\",\n",
        "    \"outil_mesure\": [\"memory_profiler\", \"timeit\"],\n",
        "    \"estimation_co2\": \"≈0.20g CO₂ économisés pour 1M objets\"\n",
        "  },\n",
        "  {\n",
        "    \"id\": \"rule_069\",\n",
        "    \"langage\": \"Python\",\n",
        "    \"contexte\": \"Aléatoire\",\n",
        "    \"anti_pattern\": \"Reseeding répété des générateurs aléatoires dans les boucles\",\n",
        "    \"advice\": \"Initialiser les graines une fois au début du run.\",\n",
        "    \"exemple_avant\": \"import random\\nfor i in range(n):\\n    random.seed(123)\\n    do(random.random())\",\n",
        "    \"exemple_apres\": \"import random\\nrandom.seed(123)\\nfor i in range(n):\\n    do(random.random())\",\n",
        "    \"gain_cpu\": \"-15%\",\n",
        "    \"gain_ram\": \"-0%\",\n",
        "    \"outil_mesure\": [\"memory_profiler\", \"timeit\"],\n",
        "    \"estimation_co2\": \"≈0.04g CO₂ économisés pour 1M itérations\"\n",
        "  },\n",
        "  {\n",
        "    \"id\": \"rule_070\",\n",
        "    \"langage\": \"Python\",\n",
        "    \"contexte\": \"Statistiques vectorisées\",\n",
        "    \"anti_pattern\": \"Calculs de stats élément par élément dans des boucles Python\",\n",
        "    \"advice\": \"Utiliser numpy.mean/std ou les méthodes pandas.\",\n",
        "    \"exemple_avant\": \"s = 0\\nfor x in arr:\\n    s += x\\nmean = s / len(arr)\",\n",
        "    \"exemple_apres\": \"import numpy as np\\nmean = np.mean(arr)\",\n",
        "    \"gain_cpu\": \"-75%\",\n",
        "    \"gain_ram\": \"-15%\",\n",
        "    \"outil_mesure\": [\"memory_profiler\", \"timeit\"],\n",
        "    \"estimation_co2\": \"≈0.28g CO₂ économisés pour 1M éléments\"\n",
        "  },\n",
        "  {\n",
        "    \"id\": \"rule_071\",\n",
        "    \"langage\": \"Python\",\n",
        "    \"contexte\": \"Parsing HTML\",\n",
        "    \"anti_pattern\": \"Parsing HTML avec regex ou split manuels\",\n",
        "    \"advice\": \"Utiliser lxml ou BeautifulSoup pour parser de manière robuste.\",\n",
        "    \"exemple_avant\": \"import re\\nlinks = re.findall(r'<a href=\\\"(.*?)\\\"', html)\",\n",
        "    \"exemple_apres\": \"from bs4 import BeautifulSoup\\nsoup = BeautifulSoup(html, 'html.parser')\\nlinks = [a['href'] for a in soup.find_all('a', href=True)]\",\n",
        "    \"gain_cpu\": \"-20%\",\n",
        "    \"gain_ram\": \"-10%\",\n",
        "    \"outil_mesure\": [\"memory_profiler\", \"timeit\"],\n",
        "    \"estimation_co2\": \"≈0.10g CO₂ économisés pour 1M balises\"\n",
        "  },\n",
        "  {\n",
        "    \"id\": \"rule_072\",\n",
        "    \"langage\": \"Python\",\n",
        "    \"contexte\": \"Itérateurs et zip\",\n",
        "    \"anti_pattern\": \"Création de grosses listes intermédiaires avec list(zip(...))\",\n",
        "    \"advice\": \"Itérer directement sur zip (générateur) ou utiliser itertools.\",\n",
        "    \"exemple_avant\": \"pairs = list(zip(a, b))\\nfor x, y in pairs:\\n    use(x, y)\",\n",
        "    \"exemple_apres\": \"for x, y in zip(a, b):\\n    use(x, y)\",\n",
        "    \"gain_cpu\": \"-10%\",\n",
        "    \"gain_ram\": \"-40%\",\n",
        "    \"outil_mesure\": [\"memory_profiler\", \"timeit\"],\n",
        "    \"estimation_co2\": \"≈0.12g CO₂ économisés pour 1M paires\"\n",
        "  },\n",
        "  {\n",
        "    \"id\": \"rule_073\",\n",
        "    \"langage\": \"Python\",\n",
        "    \"contexte\": \"Dictionnaires\",\n",
        "    \"anti_pattern\": \"try/except KeyError répété pour accéder à des clés manquantes\",\n",
        "    \"advice\": \"Utiliser dict.get avec valeur par défaut.\",\n",
        "    \"exemple_avant\": \"try:\\n    v = d[k]\\nexcept KeyError:\\n    v = 0\",\n",
        "    \"exemple_apres\": \"v = d.get(k, 0)\",\n",
        "    \"gain_cpu\": \"-25%\",\n",
        "    \"gain_ram\": \"-0%\",\n",
        "    \"outil_mesure\": [\"memory_profiler\", \"timeit\"],\n",
        "    \"estimation_co2\": \"≈0.10g CO₂ économisés pour 1M accès\"\n",
        "  },\n",
        "  {\n",
        "    \"id\": \"rule_074\",\n",
        "    \"langage\": \"Python\",\n",
        "    \"contexte\": \"Validation de schémas\",\n",
        "    \"anti_pattern\": \"Reconstruction du schéma à chaque validation\",\n",
        "    \"advice\": \"Mettre en cache/instancier une fois l’objet de schéma et le réutiliser.\",\n",
        "    \"exemple_avant\": \"for doc in docs:\\n    schema = fastjsonschema.compile(spec)\\n    schema(doc)\",\n",
        "    \"exemple_apres\": \"schema = fastjsonschema.compile(spec)\\nfor doc in docs:\\n    schema(doc)\",\n",
        "    \"gain_cpu\": \"-55%\",\n",
        "    \"gain_ram\": \"-10%\",\n",
        "    \"outil_mesure\": [\"memory_profiler\", \"timeit\"],\n",
        "    \"estimation_co2\": \"≈0.22g CO₂ économisés pour 1M documents\"\n",
        "  },\n",
        "  {\n",
        "    \"id\": \"rule_075\",\n",
        "    \"langage\": \"Python\",\n",
        "    \"contexte\": \"Boucles de production\",\n",
        "    \"anti_pattern\": \"time.sleep répétés au lieu d’attentes événementielles\",\n",
        "    \"advice\": \"Utiliser des sémaphores, files d’attente ou sélecteurs d’événements.\",\n",
        "    \"exemple_avant\": \"while not ready():\\n    time.sleep(0.1)\",\n",
        "    \"exemple_apres\": \"event.wait()  # signalé par le producteur\",\n",
        "    \"gain_cpu\": \"-20%\",\n",
        "    \"gain_ram\": \"-0%\",\n",
        "    \"outil_mesure\": [\"memory_profiler\", \"timeit\"],\n",
        "    \"estimation_co2\": \"Réduction d’attente active; impact dépend du cas\"\n",
        "  },\n",
        "  {\n",
        "    \"id\": \"rule_076\",\n",
        "    \"langage\": \"Python\",\n",
        "    \"contexte\": \"Multiprocessing et mémoire\",\n",
        "    \"anti_pattern\": \"Transfert de gros objets entre processus (pickling massif)\",\n",
        "    \"advice\": \"Partager la mémoire (multiprocessing.Array/Manager) ou mapper les fichiers.\",\n",
        "    \"exemple_avant\": \"from multiprocessing import Pool\\nwith Pool() as p:\\n    p.map(work, [big_object]*n)\",\n",
        "    \"exemple_apres\": \"from multiprocessing import Pool, Manager\\nwith Manager() as m:\\n    shared = m.list(big_object)\\n    with Pool() as p:\\n        p.map(work_shared, [shared]*n)\",\n",
        "    \"gain_cpu\": \"-30%\",\n",
        "    \"gain_ram\": \"-40%\",\n",
        "    \"outil_mesure\": [\"memory_profiler\", \"timeit\"],\n",
        "    \"estimation_co2\": \"≈0.18g CO₂ économisés pour 1M items transférés\"\n",
        "  },\n",
        "  {\n",
        "    \"id\": \"rule_077\",\n",
        "    \"langage\": \"Python\",\n",
        "    \"contexte\": \"Aléatoire\",\n",
        "    \"anti_pattern\": \"Mélange manuel d’une liste par swaps dans une boucle\",\n",
        "    \"advice\": \"Utiliser random.shuffle ou numpy.random.permutation.\",\n",
        "    \"exemple_avant\": \"for i in range(len(a)):\\n    j = random.randint(0, len(a)-1)\\n    a[i], a[j] = a[j], a[i]\",\n",
        "    \"exemple_apres\": \"import random\\nrandom.shuffle(a)\",\n",
        "    \"gain_cpu\": \"-35%\",\n",
        "    \"gain_ram\": \"-0%\",\n",
        "    \"outil_mesure\": [\"memory_profiler\", \"timeit\"],\n",
        "    \"estimation_co2\": \"≈0.14g CO₂ économisés pour 1M échanges\"\n",
        "  },\n",
        "  {\n",
        "    \"id\": \"rule_078\",\n",
        "    \"langage\": \"Python\",\n",
        "    \"contexte\": \"Gestion d’exceptions\",\n",
        "    \"anti_pattern\": \"Relancer une exception en perdant la trace (raise e)\",\n",
        "    \"advice\": \"Relancer avec raise sans argument pour conserver la stack.\",\n",
        "    \"exemple_avant\": \"try:\\n    risky()\\nexcept Exception as e:\\n    log(e)\\n    raise e\",\n",
        "    \"exemple_apres\": \"try:\\n    risky()\\nexcept Exception:\\n    log('failed')\\n    raise\",\n",
        "    \"gain_cpu\": \"-0%\",\n",
        "    \"gain_ram\": \"-0%\",\n",
        "    \"outil_mesure\": [\"memory_profiler\", \"timeit\"],\n",
        "    \"estimation_co2\": \"Impact surtout sur débogage/maintenabilité\"\n",
        "  },\n",
        "  {\n",
        "    \"id\": \"rule_079\",\n",
        "    \"langage\": \"Python\",\n",
        "    \"contexte\": \"Fichiers et encodage\",\n",
        "    \"anti_pattern\": \"Écriture de fichiers texte là où le binaire suffit (surcharge d’encodage)\",\n",
        "    \"advice\": \"Écrire en mode binaire quand c’est possible.\",\n",
        "    \"exemple_avant\": \"with open('f.out','w') as f:\\n    f.write(bytes_data.decode('latin1'))\",\n",
        "    \"exemple_apres\": \"with open('f.out','wb') as f:\\n    f.write(bytes_data)\",\n",
        "    \"gain_cpu\": \"-20%\",\n",
        "    \"gain_ram\": \"-5%\",\n",
        "    \"outil_mesure\": [\"memory_profiler\", \"timeit\"],\n",
        "    \"estimation_co2\": \"≈0.10g CO₂ économisés pour 1M écritures\"\n",
        "  },\n",
        "  {\n",
        "    \"id\": \"rule_080\",\n",
        "    \"langage\": \"Python\",\n",
        "    \"contexte\": \"JSON (sérialisation)\",\n",
        "    \"anti_pattern\": \"json.dumps lent pour de très gros dumps\",\n",
        "    \"advice\": \"Utiliser orjson/rapidjson pour des dumps rapides.\",\n",
        "    \"exemple_avant\": \"import json\\nout = json.dumps(obj)\",\n",
        "    \"exemple_apres\": \"import orjson\\nout = orjson.dumps(obj)\",\n",
        "    \"gain_cpu\": \"-50%\",\n",
        "    \"gain_ram\": \"-15%\",\n",
        "    \"outil_mesure\": [\"memory_profiler\", \"timeit\"],\n",
        "    \"estimation_co2\": \"≈0.22g CO₂ économisés pour 1M objets\"\n",
        "  },\n",
        "  {\n",
        "    \"id\": \"rule_081\",\n",
        "    \"langage\": \"Python\",\n",
        "    \"contexte\": \"JSON (options)\",\n",
        "    \"anti_pattern\": \"Tri des clés (sort_keys=True) sans nécessité\",\n",
        "    \"advice\": \"Désactiver sort_keys pour réduire le CPU.\",\n",
        "    \"exemple_avant\": \"json.dumps(obj, sort_keys=True)\",\n",
        "    \"exemple_apres\": \"json.dumps(obj, sort_keys=False)\",\n",
        "    \"gain_cpu\": \"-25%\",\n",
        "    \"gain_ram\": \"-0%\",\n",
        "    \"outil_mesure\": [\"memory_profiler\", \"timeit\"],\n",
        "    \"estimation_co2\": \"≈0.10g CO₂ économisés pour 1M dumps\"\n",
        "  },\n",
        "  {\n",
        "    \"id\": \"rule_082\",\n",
        "    \"langage\": \"Python\",\n",
        "    \"contexte\": \"Hachage et dictionnaires\",\n",
        "    \"anti_pattern\": \"Implémentations personnalisées de hash dans des boucles serrées\",\n",
        "    \"advice\": \"Utiliser les hash intégrés et types immuables (tuple, frozenset).\",\n",
        "    \"exemple_avant\": \"def my_hash(x):\\n    return sum(ord(c) for c in x)\\nkeys = {my_hash(k): v for k,v in items}\",\n",
        "    \"exemple_apres\": \"keys = {hash(k): v for k,v in items}\",\n",
        "    \"gain_cpu\": \"-40%\",\n",
        "    \"gain_ram\": \"-0%\",\n",
        "    \"outil_mesure\": [\"memory_profiler\", \"timeit\"],\n",
        "    \"estimation_co2\": \"≈0.16g CO₂ économisés pour 1M hachages\"\n",
        "  },\n",
        "  {\n",
        "    \"id\": \"rule_083\",\n",
        "    \"langage\": \"Python\",\n",
        "    \"contexte\": \"I/O réseau\",\n",
        "    \"anti_pattern\": \"Requêtes HTTP synchrones séquentielles dans une boucle\",\n",
        "    \"advice\": \"Utiliser asyncio/aiohttp pour paralléliser I/O non bloquants.\",\n",
        "    \"exemple_avant\": \"import requests\\nfor url in urls:\\n    data = requests.get(url).text\\n    consume(data)\",\n",
        "    \"exemple_apres\": \"import asyncio, aiohttp\\nasync def fetch_all(urls):\\n    async with aiohttp.ClientSession() as s:\\n        tasks = [s.get(u) for u in urls]\\n        for r in await asyncio.gather(*tasks):\\n            consume(await r.text())\\nasyncio.run(fetch_all(urls))\",\n",
        "    \"gain_cpu\": \"-10%\",\n",
        "    \"gain_ram\": \"-10%\",\n",
        "    \"outil_mesure\": [\"memory_profiler\", \"timeit\"],\n",
        "    \"estimation_co2\": \"Réduction du temps d’attente réseau; CO₂ ↓ si infra partagée\"\n",
        "  },\n",
        "  {\n",
        "    \"id\": \"rule_084\",\n",
        "    \"langage\": \"Python\",\n",
        "    \"contexte\": \"Traitement d’images\",\n",
        "    \"anti_pattern\": \"Boucles pixel-par-pixel en Python pur\",\n",
        "    \"advice\": \"Vectoriser avec OpenCV/Pillow ou NumPy.\",\n",
        "    \"exemple_avant\": \"for i in range(h):\\n    for j in range(w):\\n        img[i,j] = min(img[i,j]*2, 255)\",\n",
        "    \"exemple_apres\": \"import numpy as np\\nimg = np.clip(img * 2, 0, 255)\",\n",
        "    \"gain_cpu\": \"-85%\",\n",
        "    \"gain_ram\": \"-15%\",\n",
        "    \"outil_mesure\": [\"memory_profiler\", \"timeit\"],\n",
        "    \"estimation_co2\": \"≈0.34g CO₂ économisés pour 1M pixels\"\n",
        "  },\n",
        "  {\n",
        "    \"id\": \"rule_085\",\n",
        "    \"langage\": \"Python\",\n",
        "    \"contexte\": \"Cache et mémoire\",\n",
        "    \"anti_pattern\": \"Mise en cache d’objets énormes en mémoire locale\",\n",
        "    \"advice\": \"Limiter la taille du cache, utiliser weakref ou cache externe (Redis).\",\n",
        "    \"exemple_avant\": \"CACHE = {}\\nCACHE['big'] = big_object\",\n",
        "    \"exemple_apres\": \"import weakref\\nCACHE = weakref.WeakValueDictionary()\\nCACHE['big'] = big_object\",\n",
        "    \"gain_cpu\": \"-0%\",\n",
        "    \"gain_ram\": \"-45%\",\n",
        "    \"outil_mesure\": [\"memory_profiler\", \"timeit\"],\n",
        "    \"estimation_co2\": \"Réduction des pics mémoire; impact selon taille\"\n",
        "  },\n",
        "  {\n",
        "    \"id\": \"rule_086\",\n",
        "    \"langage\": \"Python\",\n",
        "    \"contexte\": \"NumPy et copies\",\n",
        "    \"anti_pattern\": \"deepcopy de tableaux NumPy\",\n",
        "    \"advice\": \"Utiliser numpy.copy() (ou vues) au lieu de deepcopy.\",\n",
        "    \"exemple_avant\": \"from copy import deepcopy\\nb = deepcopy(a)\",\n",
        "    \"exemple_apres\": \"b = a.copy()  # ou b = a.view()\",\n",
        "    \"gain_cpu\": \"-25%\",\n",
        "    \"gain_ram\": \"-35%\",\n",
        "    \"outil_mesure\": [\"memory_profiler\", \"timeit\"],\n",
        "    \"estimation_co2\": \"≈0.16g CO₂ économisés pour 1M éléments\"\n",
        "  },\n",
        "  {\n",
        "    \"id\": \"rule_087\",\n",
        "    \"langage\": \"Python\",\n",
        "    \"contexte\": \"Graphes et algorithmes\",\n",
        "    \"anti_pattern\": \"DFS/BFS maison non optimisés\",\n",
        "    \"advice\": \"Utiliser networkx ou bibliothèques optimisées.\",\n",
        "    \"exemple_avant\": \"def bfs(g, s):\\n    ...  # implémentation Python pure\\nres = bfs(graph, start)\",\n",
        "    \"exemple_apres\": \"import networkx as nx\\nG = nx.from_dict_of_lists(graph)\\nres = list(nx.bfs_tree(G, start))\",\n",
        "    \"gain_cpu\": \"-50%\",\n",
        "    \"gain_ram\": \"-15%\",\n",
        "    \"outil_mesure\": [\"memory_profiler\", \"timeit\"],\n",
        "    \"estimation_co2\": \"≈0.22g CO₂ économisés pour 1M arêtes\"\n",
        "  },\n",
        "  {\n",
        "    \"id\": \"rule_088\",\n",
        "    \"langage\": \"Python\",\n",
        "    \"contexte\": \"NumPy (reshape)\",\n",
        "    \"anti_pattern\": \"Reformatage de tableaux via boucles et affectations manuelles\",\n",
        "    \"advice\": \"Utiliser numpy.reshape/ravel pour réorganiser sans copier si possible.\",\n",
        "    \"exemple_avant\": \"b = np.empty((n*m,))\\nk = 0\\nfor i in range(n):\\n    for j in range(m):\\n        b[k] = a[i,j]; k += 1\",\n",
        "    \"exemple_apres\": \"b = a.ravel()\",\n",
        "    \"gain_cpu\": \"-80%\",\n",
        "    \"gain_ram\": \"-20%\",\n",
        "    \"outil_mesure\": [\"memory_profiler\", \"timeit\"],\n",
        "    \"estimation_co2\": \"≈0.30g CO₂ économisés pour 1M éléments\"\n",
        "  },\n",
        "  {\n",
        "    \"id\": \"rule_089\",\n",
        "    \"langage\": \"Python\",\n",
        "    \"contexte\": \"JSON (parsing)\",\n",
        "    \"anti_pattern\": \"json.loads appelé de manière répétée sur les mêmes chaînes\",\n",
        "    \"advice\": \"Parser une fois, réutiliser les objets ou pré-décoder en amont.\",\n",
        "    \"exemple_avant\": \"for s in strings:\\n    obj = json.loads(s)\\n    use(obj)\\n# plus tard\\nfor s in strings:\\n    obj = json.loads(s)\\n    use2(obj)\",\n",
        "    \"exemple_apres\": \"decoded = [json.loads(s) for s in strings]\\nfor obj in decoded:\\n    use(obj)\\nfor obj in decoded:\\n    use2(obj)\",\n",
        "    \"gain_cpu\": \"-45%\",\n",
        "    \"gain_ram\": \"-10%\",\n",
        "    \"outil_mesure\": [\"memory_profiler\", \"timeit\"],\n",
        "    \"estimation_co2\": \"≈0.18g CO₂ économisés pour 1M objets\"\n",
        "  },\n",
        "  {\n",
        "    \"id\": \"rule_090\",\n",
        "    \"langage\": \"Python\",\n",
        "    \"contexte\": \"SQLAlchemy et bases de données\",\n",
        "    \"anti_pattern\": \"Plusieurs requêtes dans une boucle pour simuler une jointure\",\n",
        "    \"advice\": \"Utiliser des join/outerjoin SQLAlchemy et récupérer en une fois.\",\n",
        "    \"exemple_avant\": \"for u in session.query(User).all():\\n    addr = session.query(Address).filter(Address.user_id==u.id).first()\\n    use(u, addr)\",\n",
        "    \"exemple_apres\": \"q = session.query(User, Address).join(Address, User.id==Address.user_id)\\nfor u, addr in q:\\n    use(u, addr)\",\n",
        "    \"gain_cpu\": \"-55%\",\n",
        "    \"gain_ram\": \"-20%\",\n",
        "    \"outil_mesure\": [\"memory_profiler\", \"timeit\"],\n",
        "    \"estimation_co2\": \"≈0.24g CO₂ économisés pour 1M lignes jointes\"\n",
        "  },\n",
        "    {\n",
        "    \"id\": \"rule_091\",\n",
        "    \"langage\": \"Python\",\n",
        "    \"contexte\": \"XML (sérialisation)\",\n",
        "    \"anti_pattern\": \"Sérialisation XML lente avec ElementTree par défaut\",\n",
        "    \"advice\": \"Utiliser cElementTree ou lxml pour une sérialisation plus rapide.\",\n",
        "    \"exemple_avant\": \"import xml.etree.ElementTree as ET\\nroot = ET.Element('root'); ET.SubElement(root,'x').text='1'\\nxml = ET.tostring(root)\",\n",
        "    \"exemple_apres\": \"from lxml import etree as ET\\nroot = ET.Element('root'); ET.SubElement(root,'x').text='1'\\nxml = ET.tostring(root, encoding='utf-8')\",\n",
        "    \"gain_cpu\": \"-30%\",\n",
        "    \"gain_ram\": \"-10%\",\n",
        "    \"outil_mesure\": [\"memory_profiler\", \"timeit\"],\n",
        "    \"estimation_co2\": \"≈0.12g CO₂ économisés pour 1M nœuds\"\n",
        "  },\n",
        "  {\n",
        "    \"id\": \"rule_092\",\n",
        "    \"langage\": \"Python\",\n",
        "    \"contexte\": \"pandas (indexation)\",\n",
        "    \"anti_pattern\": \"Indexation chaînée (chained indexing) provoquant des copies cachées\",\n",
        "    \"advice\": \"Utiliser .loc/.iloc pour des sélections explicites et sans copie inutile.\",\n",
        "    \"exemple_avant\": \"df[df['a']>0]['b'] = 1\",\n",
        "    \"exemple_apres\": \"df.loc[df['a']>0, 'b'] = 1\",\n",
        "    \"gain_cpu\": \"-25%\",\n",
        "    \"gain_ram\": \"-40%\",\n",
        "    \"outil_mesure\": [\"memory_profiler\", \"timeit\"],\n",
        "    \"estimation_co2\": \"≈0.18g CO₂ économisés pour 1M lignes\"\n",
        "  },\n",
        "  {\n",
        "    \"id\": \"rule_093\",\n",
        "    \"langage\": \"Python\",\n",
        "    \"contexte\": \"pandas (élément par élément)\",\n",
        "    \"anti_pattern\": \"Utilisation excessive de DataFrame.applymap pour des transformations élémentaires\",\n",
        "    \"advice\": \"Privilégier les opérations vectorisées ou .where/.mask.\",\n",
        "    \"exemple_avant\": \"df = df.applymap(lambda x: x*2)\",\n",
        "    \"exemple_apres\": \"df = df * 2\",\n",
        "    \"gain_cpu\": \"-70%\",\n",
        "    \"gain_ram\": \"-20%\",\n",
        "    \"outil_mesure\": [\"memory_profiler\", \"timeit\"],\n",
        "    \"estimation_co2\": \"≈0.26g CO₂ économisés pour 1M cellules\"\n",
        "  },\n",
        "  {\n",
        "    \"id\": \"rule_094\",\n",
        "    \"langage\": \"Python\",\n",
        "    \"contexte\": \"Données creuses (sparse)\",\n",
        "    \"anti_pattern\": \"Utilisation de matrices denses pour des données clairsemées\",\n",
        "    \"advice\": \"Utiliser scipy.sparse (CSR/CSC) pour stocker et calculer.\",\n",
        "    \"exemple_avant\": \"import numpy as np\\nA = np.zeros((n,n)); A[i,j] = 1\",\n",
        "    \"exemple_apres\": \"from scipy.sparse import csr_matrix\\nA = csr_matrix((data, (row, col)), shape=(n,n))\",\n",
        "    \"gain_cpu\": \"-50%\",\n",
        "    \"gain_ram\": \"-80%\",\n",
        "    \"outil_mesure\": [\"memory_profiler\", \"timeit\"],\n",
        "    \"estimation_co2\": \"Forte baisse de mémoire → CO₂ ↓ significativement\"\n",
        "  },\n",
        "  {\n",
        "    \"id\": \"rule_095\",\n",
        "    \"langage\": \"Python\",\n",
        "    \"contexte\": \"Filtrage de données\",\n",
        "    \"anti_pattern\": \"Boucles manuelles pour filtrer des listes/DataFrames\",\n",
        "    \"advice\": \"Utiliser des compréhensions de liste ou pandas.query.\",\n",
        "    \"exemple_avant\": \"out = []\\nfor x in xs:\\n    if cond(x):\\n        out.append(x)\",\n",
        "    \"exemple_apres\": \"out = [x for x in xs if cond(x)]  # ou df.query('a>0 & b==1')\",\n",
        "    \"gain_cpu\": \"-30%\",\n",
        "    \"gain_ram\": \"-10%\",\n",
        "    \"outil_mesure\": [\"memory_profiler\", \"timeit\"],\n",
        "    \"estimation_co2\": \"≈0.12g CO₂ économisés pour 1M éléments\"\n",
        "  },\n",
        "  {\n",
        "    \"id\": \"rule_096\",\n",
        "    \"langage\": \"Python\",\n",
        "    \"contexte\": \"Dictionnaires\",\n",
        "    \"anti_pattern\": \"Fusion manuelle de dictionnaires via boucles imbriquées\",\n",
        "    \"advice\": \"Utiliser dict.update() ou l’unpacking {**d1, **d2}.\",\n",
        "    \"exemple_avant\": \"merged = {}\\nfor d in dicts:\\n    for k,v in d.items():\\n        merged[k] = v\",\n",
        "    \"exemple_apres\": \"merged = {}\\nfor d in dicts:\\n    merged.update(d)\\n# ou merged = {k:v for d in dicts for k,v in d.items()}\",\n",
        "    \"gain_cpu\": \"-20%\",\n",
        "    \"gain_ram\": \"-5%\",\n",
        "    \"outil_mesure\": [\"memory_profiler\", \"timeit\"],\n",
        "    \"estimation_co2\": \"≈0.08g CO₂ économisés pour 1M clés\"\n",
        "  },\n",
        "  {\n",
        "    \"id\": \"rule_097\",\n",
        "    \"langage\": \"Python\",\n",
        "    \"contexte\": \"CSV (lecture)\",\n",
        "    \"anti_pattern\": \"Laisser auto-détecter le séparateur sur de gros fichiers\",\n",
        "    \"advice\": \"Définir explicitement le délimiteur pour éviter le surcoût.\",\n",
        "    \"exemple_avant\": \"import pandas as pd\\ndf = pd.read_csv('data.txt')\",\n",
        "    \"exemple_apres\": \"import pandas as pd\\ndf = pd.read_csv('data.txt', sep='\\\\t')\",\n",
        "    \"gain_cpu\": \"-25%\",\n",
        "    \"gain_ram\": \"-0%\",\n",
        "    \"outil_mesure\": [\"memory_profiler\", \"timeit\"],\n",
        "    \"estimation_co2\": \"≈0.10g CO₂ économisés pour 1M lignes\"\n",
        "  },\n",
        "  {\n",
        "    \"id\": \"rule_098\",\n",
        "    \"langage\": \"Python\",\n",
        "    \"contexte\": \"pandas (mémoire)\",\n",
        "    \"anti_pattern\": \"Colonnes string répétitives stockées en dtype object\",\n",
        "    \"advice\": \"Utiliser le dtype 'category' pour réduire la mémoire.\",\n",
        "    \"exemple_avant\": \"df['city'] = df['city'].astype(object)\",\n",
        "    \"exemple_apres\": \"df['city'] = df['city'].astype('category')\",\n",
        "    \"gain_cpu\": \"-5%\",\n",
        "    \"gain_ram\": \"-60%\",\n",
        "    \"outil_mesure\": [\"memory_profiler\", \"timeit\"],\n",
        "    \"estimation_co2\": \"Réduction forte des allocations mémoire\"\n",
        "  },\n",
        "  {\n",
        "    \"id\": \"rule_099\",\n",
        "    \"langage\": \"Python\",\n",
        "    \"contexte\": \"Flux de données\",\n",
        "    \"anti_pattern\": \"Matérialiser de grosses listes intermédiaires au lieu de générer à la volée\",\n",
        "    \"advice\": \"Utiliser des générateurs (yield) et l’itération paresseuse.\",\n",
        "    \"exemple_avant\": \"rows = [read_record(f) for f in files]\\nfor r in rows:\\n    use(r)\",\n",
        "    \"exemple_apres\": \"def iter_rows(files):\\n    for f in files:\\n        yield read_record(f)\\nfor r in iter_rows(files):\\n    use(r)\",\n",
        "    \"gain_cpu\": \"-15%\",\n",
        "    \"gain_ram\": \"-50%\",\n",
        "    \"outil_mesure\": [\"memory_profiler\", \"timeit\"],\n",
        "    \"estimation_co2\": \"Moins de pics mémoire → CO₂ ↓\"\n",
        "  },\n",
        "  {\n",
        "    \"id\": \"rule_100\",\n",
        "    \"langage\": \"Python\",\n",
        "    \"contexte\": \"NumPy (précision)\",\n",
        "    \"anti_pattern\": \"Utiliser float64/int64 par défaut sans nécessité\",\n",
        "    \"advice\": \"Réduire la précision (float32/int32) quand c’est acceptable.\",\n",
        "    \"exemple_avant\": \"a = np.array(vals, dtype=np.float64)\",\n",
        "    \"exemple_apres\": \"a = np.array(vals, dtype=np.float32)\",\n",
        "    \"gain_cpu\": \"-20%\",\n",
        "    \"gain_ram\": \"-50%\",\n",
        "    \"outil_mesure\": [\"memory_profiler\", \"timeit\"],\n",
        "    \"estimation_co2\": \"Moitié de mémoire sur colonnes flottantes\"\n",
        "  },\n",
        "  {\n",
        "    \"id\": \"rule_101\",\n",
        "    \"langage\": \"Python\",\n",
        "    \"contexte\": \"CSV (écriture)\",\n",
        "    \"anti_pattern\": \"Quoting par défaut coûteux ou incohérent\",\n",
        "    \"advice\": \"Contrôler explicitement quoting pour limiter le travail sur les chaînes.\",\n",
        "    \"exemple_avant\": \"import csv\\nw = csv.writer(open('f.csv','w', newline=''))\\nw.writerows(rows)\",\n",
        "    \"exemple_apres\": \"import csv\\nw = csv.writer(open('f.csv','w', newline=''), quoting=csv.QUOTE_MINIMAL)\\nw.writerows(rows)\",\n",
        "    \"gain_cpu\": \"-15%\",\n",
        "    \"gain_ram\": \"-5%\",\n",
        "    \"outil_mesure\": [\"memory_profiler\", \"timeit\"],\n",
        "    \"estimation_co2\": \"≈0.06g CO₂ économisés pour 1M lignes\"\n",
        "  },\n",
        "  {\n",
        "    \"id\": \"rule_102\",\n",
        "    \"langage\": \"Python\",\n",
        "    \"contexte\": \"Chaînes de caractères\",\n",
        "    \"anti_pattern\": \"Assemblage de très nombreuses chaînes via '+' dans une boucle\",\n",
        "    \"advice\": \"Utiliser str.join sur une liste d’éléments.\",\n",
        "    \"exemple_avant\": \"s = ''\\nfor p in parts:\\n    s += p\",\n",
        "    \"exemple_apres\": \"s = ''.join(parts)\",\n",
        "    \"gain_cpu\": \"-65%\",\n",
        "    \"gain_ram\": \"-20%\",\n",
        "    \"outil_mesure\": [\"memory_profiler\", \"timeit\"],\n",
        "    \"estimation_co2\": \"≈0.24g CO₂ économisés pour 1M concaténations\"\n",
        "  },\n",
        "  {\n",
        "    \"id\": \"rule_103\",\n",
        "    \"langage\": \"Python\",\n",
        "    \"contexte\": \"pandas (tri)\",\n",
        "    \"anti_pattern\": \"Appels de tri répétés ou sans clé précise\",\n",
        "    \"advice\": \"Utiliser sort_values avec clés précises et éviter les tris multiples.\",\n",
        "    \"exemple_avant\": \"df = df.sort_values(by=list(df.columns))\\n# puis retrié plusieurs fois\",\n",
        "    \"exemple_apres\": \"df = df.sort_values(by=['date','id'])  # une seule fois\",\n",
        "    \"gain_cpu\": \"-50%\",\n",
        "    \"gain_ram\": \"-15%\",\n",
        "    \"outil_mesure\": [\"memory_profiler\", \"timeit\"],\n",
        "    \"estimation_co2\": \"≈0.20g CO₂ économisés pour 1M lignes\"\n",
        "  },\n",
        "  {\n",
        "    \"id\": \"rule_104\",\n",
        "    \"langage\": \"Python\",\n",
        "    \"contexte\": \"Algèbre linéaire\",\n",
        "    \"anti_pattern\": \"Inversion explicite de matrice avec np.linalg.inv pour résoudre Ax=b\",\n",
        "    \"advice\": \"Utiliser np.linalg.solve (plus stable et plus rapide).\",\n",
        "    \"exemple_avant\": \"x = np.linalg.inv(A) @ b\",\n",
        "    \"exemple_apres\": \"x = np.linalg.solve(A, b)\",\n",
        "    \"gain_cpu\": \"-35%\",\n",
        "    \"gain_ram\": \"-10%\",\n",
        "    \"outil_mesure\": [\"memory_profiler\", \"timeit\"],\n",
        "    \"estimation_co2\": \"≈0.14g CO₂ économisés pour 1M systèmes\"\n",
        "  },\n",
        "  {\n",
        "    \"id\": \"rule_105\",\n",
        "    \"langage\": \"Python\",\n",
        "    \"contexte\": \"Sérialisation de données\",\n",
        "    \"anti_pattern\": \"Utiliser CSV pour des datasets volumineux\",\n",
        "    \"advice\": \"Préférer des formats binaires colonnes (parquet, feather).\",\n",
        "    \"exemple_avant\": \"df.to_csv('data.csv', index=False)\",\n",
        "    \"exemple_apres\": \"df.to_parquet('data.parquet')  # ou .to_feather(...)\",\n",
        "    \"gain_cpu\": \"-30%\",\n",
        "    \"gain_ram\": \"-25%\",\n",
        "    \"outil_mesure\": [\"memory_profiler\", \"timeit\"],\n",
        "    \"estimation_co2\": \"I/O réduit → CO₂ ↓ notable sur gros volumes\"\n",
        "  },\n",
        "  {\n",
        "    \"id\": \"rule_106\",\n",
        "    \"langage\": \"Python\",\n",
        "    \"contexte\": \"Téléchargements HTTP\",\n",
        "    \"anti_pattern\": \"Télécharger un gros fichier en une seule fois en mémoire\",\n",
        "    \"advice\": \"Utiliser requests avec stream=True et iter_content par chunks.\",\n",
        "    \"exemple_avant\": \"r = requests.get(url)\\nopen('f.bin','wb').write(r.content)\",\n",
        "    \"exemple_apres\": \"with requests.get(url, stream=True) as r, open('f.bin','wb') as f:\\n    for chunk in r.iter_content(chunk_size=1<<20):\\n        if chunk: f.write(chunk)\",\n",
        "    \"gain_cpu\": \"-5%\",\n",
        "    \"gain_ram\": \"-80%\",\n",
        "    \"outil_mesure\": [\"memory_profiler\", \"timeit\"],\n",
        "    \"estimation_co2\": \"Pics mémoire réduits, I/O plus stables\"\n",
        "  },\n",
        "  {\n",
        "    \"id\": \"rule_107\",\n",
        "    \"langage\": \"Python\",\n",
        "    \"contexte\": \"Ensembles\",\n",
        "    \"anti_pattern\": \"Intersection manuelle par boucles\",\n",
        "    \"advice\": \"Utiliser set.intersection pour calculer l’intersection efficacement.\",\n",
        "    \"exemple_avant\": \"common = []\\nfor x in A:\\n    if x in B:\\n        common.append(x)\",\n",
        "    \"exemple_apres\": \"common = set(A).intersection(B)\",\n",
        "    \"gain_cpu\": \"-70%\",\n",
        "    \"gain_ram\": \"-20%\",\n",
        "    \"outil_mesure\": [\"memory_profiler\", \"timeit\"],\n",
        "    \"estimation_co2\": \"≈0.26g CO₂ économisés pour 1M comparaisons\"\n",
        "  },\n",
        "  {\n",
        "    \"id\": \"rule_108\",\n",
        "    \"langage\": \"Python\",\n",
        "    \"contexte\": \"Logging en production\",\n",
        "    \"anti_pattern\": \"Handlers de log sans rotation menant à des fichiers énormes\",\n",
        "    \"advice\": \"Utiliser RotatingFileHandler/TimedRotatingFileHandler et niveaux adaptés.\",\n",
        "    \"exemple_avant\": \"logging.basicConfig(filename='app.log', level=logging.INFO)\",\n",
        "    \"exemple_apres\": \"from logging.handlers import RotatingFileHandler\\nh = RotatingFileHandler('app.log', maxBytes=10_000_000, backupCount=5)\\nlogger = logging.getLogger('app'); logger.addHandler(h)\",\n",
        "    \"gain_cpu\": \"-5%\",\n",
        "    \"gain_ram\": \"-5%\",\n",
        "    \"outil_mesure\": [\"memory_profiler\", \"timeit\"],\n",
        "    \"estimation_co2\": \"I/O disque mieux maîtrisé sur longue durée\"\n",
        "  },\n",
        "  {\n",
        "    \"id\": \"rule_109\",\n",
        "    \"langage\": \"Python\",\n",
        "    \"contexte\": \"CSV → pandas\",\n",
        "    \"anti_pattern\": \"Laisser pandas créer un index par défaut inutile\",\n",
        "    \"advice\": \"Spécifier index_col pour éviter une colonne supplémentaire.\",\n",
        "    \"exemple_avant\": \"df = pd.read_csv('data.csv')\",\n",
        "    \"exemple_apres\": \"df = pd.read_csv('data.csv', index_col='id')\",\n",
        "    \"gain_cpu\": \"-5%\",\n",
        "    \"gain_ram\": \"-15%\",\n",
        "    \"outil_mesure\": [\"memory_profiler\", \"timeit\"],\n",
        "    \"estimation_co2\": \"Moins de colonnes → mémoire et I/O réduites\"\n",
        "  },\n",
        "  {\n",
        "    \"id\": \"rule_110\",\n",
        "    \"langage\": \"Python\",\n",
        "    \"contexte\": \"Lisibilité/maintenabilité\",\n",
        "    \"anti_pattern\": \"Lambdas complexes et répétées au lieu de fonctions nommées\",\n",
        "    \"advice\": \"Définir une fonction nommée réutilisable (possiblement compilée/cachée).\",\n",
        "    \"exemple_avant\": \"df['y'] = df['x'].apply(lambda v: (v*v + 1)/3 if v>0 else 0)\",\n",
        "    \"exemple_apres\": \"def transform(v):\\n    return (v*v + 1)/3 if v>0 else 0\\ndf['y'] = df['x'].apply(transform)\",\n",
        "    \"gain_cpu\": \"-5%\",\n",
        "    \"gain_ram\": \"-0%\",\n",
        "    \"outil_mesure\": [\"memory_profiler\", \"timeit\"],\n",
        "    \"estimation_co2\": \"Impact faible; robustesse accrue\"\n",
        "  },\n",
        "  {\n",
        "    \"id\": \"rule_111\",\n",
        "    \"langage\": \"Python\",\n",
        "    \"contexte\": \"Multiprocessing (plateformes Unix)\",\n",
        "    \"anti_pattern\": \"Utiliser le start method par défaut inadapté provoquant surcoûts et copies\",\n",
        "    \"advice\": \"Privilégier 'forkserver' ou 'spawn' selon le besoin de stabilité/mémoire.\",\n",
        "    \"exemple_avant\": \"import multiprocessing as mp\\n# start method implicite\\np = mp.Pool().map(f, xs)\",\n",
        "    \"exemple_apres\": \"import multiprocessing as mp\\nmp.set_start_method('forkserver', force=True)\\nwith mp.Pool() as p:\\n    p.map(f, xs)\",\n",
        "    \"gain_cpu\": \"-10%\",\n",
        "    \"gain_ram\": \"-20%\",\n",
        "    \"outil_mesure\": [\"memory_profiler\", \"timeit\"],\n",
        "    \"estimation_co2\": \"Moins de copies inattendues → CO₂ ↓\"\n",
        "  },\n",
        "  {\n",
        "    \"id\": \"rule_112\",\n",
        "    \"langage\": \"Python\",\n",
        "    \"contexte\": \"Système/OS\",\n",
        "    \"anti_pattern\": \"Accès répétés à os.environ dans les boucles\",\n",
        "    \"advice\": \"Mettre en cache les valeurs d’environnement en variables locales.\",\n",
        "    \"exemple_avant\": \"for _ in range(n):\\n    mode = os.environ.get('APP_MODE','dev')\\n    do(mode)\",\n",
        "    \"exemple_apres\": \"mode = os.environ.get('APP_MODE','dev')\\nfor _ in range(n):\\n    do(mode)\",\n",
        "    \"gain_cpu\": \"-15%\",\n",
        "    \"gain_ram\": \"-0%\",\n",
        "    \"outil_mesure\": [\"memory_profiler\", \"timeit\"],\n",
        "    \"estimation_co2\": \"≈0.06g CO₂ économisés pour 1M accès\"\n",
        "  },\n",
        "  {\n",
        "    \"id\": \"rule_113\",\n",
        "    \"langage\": \"Python\",\n",
        "    \"contexte\": \"pandas (dtypes)\",\n",
        "    \"anti_pattern\": \"Types numériques non optimisés (int64/float64) par défaut\",\n",
        "    \"advice\": \"Downcaster les colonnes (to_numeric(..., downcast=...), astype) pour réduire la mémoire.\",\n",
        "    \"exemple_avant\": \"df['count'] = df['count'].astype('int64')\",\n",
        "    \"exemple_apres\": \"df['count'] = pd.to_numeric(df['count'], downcast='integer')\",\n",
        "    \"gain_cpu\": \"-5%\",\n",
        "    \"gain_ram\": \"-40%\",\n",
        "    \"outil_mesure\": [\"memory_profiler\", \"timeit\"],\n",
        "    \"estimation_co2\": \"Réduction mémoire substantielle sur colonnes volumineuses\"\n",
        "  }\n",
        "]\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 2. Brique 2 – Structuration & Analyse des pratiques\n",
        "# Embed KB entries for semantic search  (sanitized – no raw vectors exposed)\n",
        "# ------------------------------------------------------------\n",
        "from functools import lru_cache\n",
        "import numpy as np\n",
        "\n",
        "# Toggle: keep False to avoid storing giant vectors inside the KB\n",
        "STORE_KB_VECTORS = False\n",
        "\n",
        "@lru_cache(maxsize=4096)\n",
        "def _embed_cached(text: str, input_type: str = \"query\") -> tuple:\n",
        "    emb = client.embeddings.create(\n",
        "        model=\"nvidia/nv-embedqa-e5-v5\",\n",
        "        input=text,\n",
        "        extra_body={\"input_type\": input_type}  # must be \"query\" or \"passage\"\n",
        "    )\n",
        "    # tuple for cacheability; we won't print or store these in the KB\n",
        "    return tuple(emb.data[0].embedding)\n",
        "\n",
        "def embed_text(text: str, input_type: str = \"query\") -> np.ndarray:\n",
        "    return np.asarray(_embed_cached(text, input_type=input_type), dtype=np.float32)\n",
        "\n",
        "# Embed KB entries as \"passages\" (kept for compatibility, but gated)\n",
        "# NOTE: we do NOT store real vectors unless STORE_KB_VECTORS=True.\n",
        "for rule in knowledge_base:\n",
        "    if STORE_KB_VECTORS:\n",
        "        rule[\"vector\"] = embed_text(rule.get(\"advice\", \"\"), input_type=\"passage\")\n",
        "    else:\n",
        "        # keep the key present but lightweight to respect existing code expectations\n",
        "        rule[\"vector\"] = None\n",
        "\n",
        "# Naive similarity function (dot product)\n",
        "def cosine_similarity(vec1, vec2):\n",
        "    v1, v2 = np.asarray(vec1), np.asarray(vec2)\n",
        "    denom = (np.linalg.norm(v1) * np.linalg.norm(v2)) or 1.0\n",
        "    return float(v1.dot(v2) / denom)\n",
        "\n",
        "# keep only light rule fields in outputs\n",
        "LIGHT_RULE_KEYS = (\"id\", \"contexte\", \"anti_pattern\", \"advice\", \"gain_cpu\", \"gain_ram\", \"estimation_co2\")\n",
        "def _sanitize_rules(rules: list[dict]) -> list[dict]:\n",
        "    out = []\n",
        "    for r in rules:\n",
        "        out.append({k: r[k] for k in LIGHT_RULE_KEYS if k in r})\n",
        "    return out\n",
        "\n",
        "def retrieve_best_practices(code: str, top_k: int = 1) -> List[Dict]:\n",
        "    # Compute similarity on-the-fly; don't rely on stored vectors\n",
        "    q = embed_text(code, input_type=\"query\")\n",
        "    scored = []\n",
        "    for r in knowledge_base:\n",
        "        pv = embed_text(r.get(\"advice\", \"\"), input_type=\"passage\")\n",
        "        scored.append((cosine_similarity(q, pv), r))\n",
        "    scored.sort(key=lambda x: x[0], reverse=True)\n",
        "    return _sanitize_rules([r for _, r in scored[:top_k]])\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 3. Brique 3 – Analyse du code soumis\n",
        "# Simple AST-based check for inefficiencies\n",
        "# ------------------------------------------------------------\n",
        "def analyze_code(code: str) -> Dict:\n",
        "    analysis = {\"language\": \"python\", \"patterns\": []}\n",
        "    try:\n",
        "        tree = ast.parse(code)\n",
        "        for node in ast.walk(tree):\n",
        "            if isinstance(node, ast.For):\n",
        "                # check for += str() concatenation inside loop\n",
        "                for n in ast.walk(node):\n",
        "                    if isinstance(n, ast.AugAssign) and isinstance(n.op, ast.Add):\n",
        "                        analysis[\"patterns\"].append(\"concat string in loop\")\n",
        "    except Exception as e:\n",
        "        analysis[\"error\"] = str(e)\n",
        "    return analysis\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 3bis. Analyse AST native + Hybride (AST + Embeddings)\n",
        "# ------------------------------------------------------------\n",
        "LANGUAGE_REGEX = {\n",
        "    \"Python\": [r\"\\bdef\\b\", r\"\\bimport\\b\", r\":\\s*(#.*)?$\", r\"\\bprint\\(\", r\"\\bNone\\b\", r\"^\\s*#\"],\n",
        "    \"JavaScript\": [r\"\\bfunction\\b\", r\"=>\", r\"\\bconsole\\.log\\b\", r\"\\b(var|let|const)\\b\", r\";\\s*$\", r\"\\bimport\\s\"],\n",
        "    \"Java\": [r\"\\bpublic\\s+class\\b\", r\"\\bpublic\\s+static\\s+void\\s+main\\b\", r\"\\bSystem\\.out\\.println\\b\", r\";\\s*$\"],\n",
        "    \"C++\": [r\"#include\\s*<\", r\"\\bstd::\", r\"\\bcout\\s*<<\", r\";\\s*$\"],\n",
        "}\n",
        "\n",
        "def _language_by_regex(code: str):\n",
        "    scores = {}\n",
        "    for lang, pats in LANGUAGE_REGEX.items():\n",
        "        s = sum(1 for p in pats if re.search(p, code, flags=re.MULTILINE))\n",
        "        scores[lang] = float(s)\n",
        "    best = max(scores.items(), key=lambda x: x[1])[0] if scores else \"Unknown\"\n",
        "    maxs = max(scores.values()) or 1.0\n",
        "    return best, {k: v / maxs for k, v in scores.items()}\n",
        "\n",
        "def _detect_python_patterns(code: str):\n",
        "    patterns, tags = [], []\n",
        "    try:\n",
        "        tree = ast.parse(code)\n",
        "    except Exception:\n",
        "        tree = None\n",
        "    if tree:\n",
        "        for node in ast.walk(tree):\n",
        "            if isinstance(node, ast.For):\n",
        "                for n in ast.walk(node):\n",
        "                    if isinstance(n, ast.AugAssign) and isinstance(n.op, ast.Add):\n",
        "                        patterns.append(\"concat string in loop\")\n",
        "                        tags.append(\"strings:concat_in_loop\")\n",
        "                        break\n",
        "    if re.search(r'(^|\\s)open\\s*\\(', code) and not re.search(r'with\\s+open\\s*\\(', code):\n",
        "        patterns.append(\"file open without context manager\"); tags.append(\"io:missing_with_open\")\n",
        "    if \"import re\" in code and re.search(r'for\\s+.+:\\s*[\\s\\S]*?re\\.(search|match|findall)\\(', code, re.MULTILINE):\n",
        "        if \"re.compile(\" not in code: patterns.append(\"regex not compiled inside loop\"); tags.append(\"regex:missing_compile\")\n",
        "    if re.search(r'for\\s+\\w+\\s+in\\s+range\\(\\s*len\\(', code):\n",
        "        patterns.append(\"len(list) inside loop range\"); tags.append(\"loops:len_in_loop\")\n",
        "    if re.search(r'\\.apply\\(', code) or re.search(r'\\.iterrows\\(', code):\n",
        "        patterns.append(\"row-wise pandas apply/iterrows\"); tags.append(\"pandas:row_wise\")\n",
        "    if re.search(r'for\\s+\\w+\\s+in\\s+.+:\\s*[\\s\\S]*?\\.append\\(', code, re.MULTILINE):\n",
        "        patterns.append(\"append in large loop\"); tags.append(\"lists:append_loop\")\n",
        "    return patterns, tags\n",
        "\n",
        "def _detect_language(code: str) -> str:\n",
        "    lang, _ = _language_by_regex(code)\n",
        "    return lang\n",
        "\n",
        "def _detect_libraries(code: str, language: str) -> List[str]:\n",
        "    libs = []\n",
        "    if language == \"Python\":\n",
        "        try:\n",
        "            tree = ast.parse(code)\n",
        "            for n in ast.walk(tree):\n",
        "                if isinstance(n, ast.Import):\n",
        "                    libs += [a.name.split(\".\")[0] for a in n.names]\n",
        "                elif isinstance(n, ast.ImportFrom) and n.module:\n",
        "                    libs.append(n.module.split(\".\")[0])\n",
        "        except SyntaxError:\n",
        "            pass\n",
        "    elif language == \"JavaScript\":\n",
        "        libs += re.findall(r\"from\\s+['\\\"]([^'\\\"]+)['\\\"]\", code)\n",
        "        libs += re.findall(r\"require\\(\\s*['\\\"]([^'\\\"]+)['\\\"]\\s*\\)\", code)\n",
        "    return sorted(set(libs))\n",
        "\n",
        "def analyze_native_ast(code: str) -> Dict:\n",
        "    lang = _detect_language(code)\n",
        "    libs = _detect_libraries(code, lang)\n",
        "    _, tags = _detect_python_patterns(code) if lang == \"Python\" else ([], [])\n",
        "    type_map = {\n",
        "        \"strings\": \"Chaînes de caractères\",\n",
        "        \"loops\": \"Algorithmes et boucles\",\n",
        "        \"regex\": \"Regex et parsing\",\n",
        "        \"io\": \"Fichiers et I/O\",\n",
        "        \"pandas\": \"pandas\",\n",
        "        \"numpy\": \"NumPy\",\n",
        "        \"lists\": \"Structures de données\",\n",
        "    }\n",
        "    opti_types = sorted({v for k, v in type_map.items() if any(t.startswith(k + \":\") for t in tags)})\n",
        "    return {\n",
        "        \"approach\": \"Analyse AST native\",\n",
        "        \"language\": lang,\n",
        "        \"libraries\": libs,\n",
        "        \"optimization_types\": opti_types,\n",
        "        \"motifs\": sorted(set(tags)),\n",
        "    }\n",
        "\n",
        "def analyze_hybrid_ast_embeddings(code: str, top_k: int = 3) -> Dict:\n",
        "    base = analyze_native_ast(code)\n",
        "    recs = retrieve_best_practices(code, top_k=top_k)  # sanitized rules only\n",
        "    base.update({\n",
        "        \"approach\": \"Modèle hybride AST + Embeddings\",\n",
        "        \"recommendations\": recs\n",
        "    })\n",
        "    return base\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 4. Brique 4 – LLM Suggestion & Optimization (NVIDIA NIM)\n",
        "# (no streaming, no 'thinking')\n",
        "# ------------------------------------------------------------\n",
        "def suggest_optimization(user_code: str) -> str:\n",
        "    # Step A: Analyze code\n",
        "    analysis = analyze_code(user_code)\n",
        "    # Step B: Retrieve KB rules (sanitized)\n",
        "    best_rules = retrieve_best_practices(user_code, top_k=1)\n",
        "    # Step C: Build RAG prompt\n",
        "    rag_prompt = f\"\"\"\n",
        "    User code:\n",
        "    {user_code}\n",
        "\n",
        "    Detected patterns: {analysis.get(\"patterns\")}\n",
        "\n",
        "    Relevant Green practices:\n",
        "    {json.dumps(best_rules, indent=2, ensure_ascii=False)}\n",
        "\n",
        "    Task: Rewrite the code in a more sustainable way,\n",
        "    explain the optimizations and their environmental benefits.\n",
        "    \"\"\"\n",
        "    # Step D: Query NVIDIA NIM model\n",
        "    completion = client.chat.completions.create(\n",
        "        model=\"qwen/qwen3-235b-a22b\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are GreenCoder, an assistant that improves code for sustainability.\"},\n",
        "            {\"role\": \"user\", \"content\": rag_prompt}\n",
        "        ],\n",
        "        temperature=0.2,\n",
        "        top_p=0.7,\n",
        "        max_tokens=1024\n",
        "    )\n",
        "    # Get output\n",
        "    final_answer = completion.choices[0].message.content\n",
        "    print(final_answer)\n",
        "    return final_answer\n",
        "\n",
        "# ==================================================================================================\n",
        "# MÉTHODE 1 : Regex + Heuristiques (kept; sanitize kb_rules)\n",
        "# ==================================================================================================\n",
        "def _kb_match_rules_keyword(knowledge_base: List[Dict], code: str, max_rules: int = 5) -> List[Dict]:\n",
        "    signals = {\n",
        "        \"concat\": [\"+=\", \"''.join\", \"join(\"],\n",
        "        \"regex\": [\"re.\", \"regex\", \"re.compile(\"],\n",
        "        \"pandas\": [\"pandas\", \"pd.\", \".apply(\", \".iterrows(\"],\n",
        "        \"numpy\": [\"numpy\", \"np.\"],\n",
        "        \"io\": [\"open(\", \"write(\", \"read(\"],\n",
        "        \"set\": [\" set(\", \"dict(\", \"defaultdict(\"],\n",
        "        \"cache\": [\"lru_cache\", \"@lru_cache\"],\n",
        "        \"copy\": [\"deepcopy(\", \".copy(\"],\n",
        "    }\n",
        "    low_code = code.lower()\n",
        "    scored = []\n",
        "    for r in knowledge_base:\n",
        "        text = \" \".join([str(r.get(\"advice\") or r.get(\"advice\") or \"\"),\n",
        "                         str(r.get(\"anti_pattern\") or \"\"),\n",
        "                         str(r.get(\"contexte\") or \"\")]).lower()\n",
        "        s = 0\n",
        "        for k, keys in signals.items():\n",
        "            if any(kx.lower() in low_code for kx in keys) and k in text: s += 2\n",
        "            if k in text: s += 1\n",
        "        if any(w in text for w in [\"boucle\",\"vectoris\",\"pandas\",\"numpy\",\"json\",\"io\",\"regex\"]) and \\\n",
        "           any(w in low_code for w in [\"for \", \"while \", \"apply(\", \"np.\", \"pd.\", \"open(\"]):\n",
        "            s += 1\n",
        "        scored.append((s, r))\n",
        "    scored.sort(key=lambda x: x[0], reverse=True)\n",
        "    return _sanitize_rules([r for s, r in scored if s > 0][:max_rules])\n",
        "\n",
        "def method1_regex_heuristics(code: str, knowledge_base: List[Dict]) -> Dict:\n",
        "    language, lang_scores = _language_by_regex(code)\n",
        "    patterns, tags = _detect_python_patterns(code)\n",
        "    matched_rules = _kb_match_rules_keyword(knowledge_base, code, max_rules=6)\n",
        "    type_map = {\n",
        "        \"strings\": \"Chaînes de caractères\",\n",
        "        \"loops\": \"Algorithmes et boucles\",\n",
        "        \"regex\": \"Regex et parsing\",\n",
        "        \"io\": \"Fichiers et I/O\",\n",
        "        \"pandas\": \"pandas\",\n",
        "        \"numpy\": \"NumPy\",\n",
        "        \"lists\": \"Structures de données\",\n",
        "    }\n",
        "    opti_types = sorted({v for k, v in type_map.items() if any(t.startswith(k + \":\") for t in tags)})\n",
        "    return {\n",
        "        \"language\": language,\n",
        "        \"language_scores\": lang_scores,\n",
        "        \"optimization_types\": opti_types,\n",
        "        \"tags\": sorted(set(tags + patterns)),\n",
        "        \"kb_rules\": matched_rules,   # sanitized\n",
        "        \"inconsistencies\": []\n",
        "    }\n",
        "\n",
        "# ==================================================================================================\n",
        "# MÉTHODE 4 : Classification supervisée (RandomForest)  (kept; sanitize kb_rules)\n",
        "# ==================================================================================================\n",
        "def _get_rf_models():\n",
        "    try:\n",
        "        from sklearn.pipeline import Pipeline\n",
        "        from sklearn.ensemble import RandomForestClassifier\n",
        "        from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(\"Installe scikit-learn: pip install scikit-learn\") from e\n",
        "\n",
        "    LANG_DOCS = [\n",
        "        \"def f(x):\\n    import math\\n    return x+1\\nprint('ok')\",\n",
        "        \"function f(x){ console.log(x+1); }\",\n",
        "        \"public class A { public static void main(String[] a){ System.out.println(1); } }\",\n",
        "        \"#include <iostream>\\nint main(){ std::cout<<1; }\",\n",
        "    ]\n",
        "    LANG_LBLS = [\"Python\", \"JavaScript\", \"Java\", \"C++\"]\n",
        "\n",
        "    OPTI_DOCS = [\n",
        "        \"s='';\\nfor x in data:\\n    s += str(x)\",\n",
        "        \"with open(p,'w') as f:\\n    f.write('x')\",\n",
        "        \"for i in range(len(lst)):\\n    total+=lst[i]\",\n",
        "        \"import pandas as pd\\ndf.apply(lambda r: r.x+1, axis=1)\",\n",
        "        \"import re\\nfor l in lines:\\n    if re.search('[0-9]+', l): pass\",\n",
        "        \"import numpy as np\\nout = a * 2\",\n",
        "    ]\n",
        "    OPTI_LBLS = [\"Chaînes de caractères\", \"Fichiers et I/O\", \"Algorithmes et boucles\",\n",
        "                 \"pandas\", \"Regex et parsing\", \"NumPy\"]\n",
        "\n",
        "    from sklearn.pipeline import Pipeline\n",
        "    from sklearn.ensemble import RandomForestClassifier\n",
        "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "    lang_model = Pipeline([\n",
        "        (\"tfidf\", TfidfVectorizer(analyzer=\"char\", ngram_range=(3,5), min_df=1)),\n",
        "        (\"clf\", RandomForestClassifier(n_estimators=300, random_state=42, n_jobs=-1))\n",
        "    ]).fit(LANG_DOCS, LANG_LBLS)\n",
        "\n",
        "    vocab = [\n",
        "        \"for\", \"while\", \"append(\", \"apply(\", \"iterrows(\", \"join(\", \"re.compile(\", \"open(\", \"heapq\",\n",
        "        \"np.\", \"pandas\", \"set(\", \"dict(\", \"lru_cache\", \"deepcopy(\", \".copy(\", \"len(\", \"json.loads(\", \"csv.\"\n",
        "    ]\n",
        "    opti_model = Pipeline([\n",
        "        (\"tfidf\", TfidfVectorizer(vocabulary=vocab, lowercase=False)),\n",
        "        (\"clf\", RandomForestClassifier(n_estimators=300, random_state=42, n_jobs=-1))\n",
        "    ]).fit(OPTI_DOCS, OPTI_LBLS)\n",
        "\n",
        "    return lang_model, opti_model\n",
        "\n",
        "def method4_supervised_rf(code: str, knowledge_base: List[Dict]) -> Dict:\n",
        "    lang_model, opti_model = _get_rf_models()\n",
        "    lang_pred = lang_model.predict([code])[0]\n",
        "    if hasattr(lang_model, \"predict_proba\"):\n",
        "        classes = list(lang_model.classes_)\n",
        "        probs = lang_model.predict_proba([code])[0]\n",
        "        lang_scores = {c: float(p) for c, p in zip(classes, probs)}\n",
        "    else:\n",
        "        lang_scores = {lang_pred: 1.0}\n",
        "    opti_pred = opti_model.predict([code])[0]\n",
        "    _, tags = _detect_python_patterns(code)\n",
        "    matched_rules = _kb_match_rules_keyword(knowledge_base, code, max_rules=8)\n",
        "    return {\n",
        "        \"language\": lang_pred,\n",
        "        \"language_scores\": lang_scores,\n",
        "        \"optimization_types\": [opti_pred],\n",
        "        \"tags\": sorted(set(tags)),\n",
        "        \"kb_rules\": matched_rules,   # sanitized\n",
        "        \"inconsistencies\": []\n",
        "    }\n",
        "\n",
        "# ==================================================================================================\n",
        "# MÉTHODE 5 : Embedding + Clustering (SBERT + KMeans)  (kept; sanitized kb_rules)\n",
        "# ==================================================================================================\n",
        "def method5_embed_cluster(code: str, knowledge_base: List[Dict], top_k: int = 12, n_clusters: int = 3) -> Dict:\n",
        "    try:\n",
        "        from sentence_transformers import SentenceTransformer\n",
        "        from sklearn.cluster import KMeans\n",
        "        from sklearn.metrics.pairwise import cosine_similarity as cos_sim\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(\"Installe: pip install sentence-transformers scikit-learn\") from e\n",
        "\n",
        "    model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "    kb_texts = [\n",
        "        f\"{r.get('contexte','')} | {r.get('anti_pattern','')} | {(r.get('advice') or r.get('advice') or '')}\"\n",
        "        for r in knowledge_base\n",
        "    ]\n",
        "    kb_vecs = model.encode(kb_texts, normalize_embeddings=True)\n",
        "    code_vec = model.encode([code], normalize_embeddings=True)\n",
        "\n",
        "    sims = cos_sim(code_vec, kb_vecs).ravel()\n",
        "    idx = np.argsort(-sims)[:top_k]\n",
        "    top_rules = [knowledge_base[i] for i in idx]\n",
        "    top_vecs = kb_vecs[idx]\n",
        "\n",
        "    k = max(1, min(n_clusters, len(top_rules)))\n",
        "    kmeans = KMeans(n_clusters=k, n_init=10, random_state=42)\n",
        "    labels = kmeans.fit_predict(top_vecs)\n",
        "\n",
        "    ctx = []\n",
        "    tags = []\n",
        "    for lab, rule in zip(labels, top_rules):\n",
        "        if rule.get(\"contexte\"): ctx.append(rule[\"contexte\"])\n",
        "        if rule.get(\"anti_pattern\"): tags.append(f\"kb:{rule['id']}:{rule['anti_pattern'][:40]}\")\n",
        "        else: tags.append(f\"kb:{rule['id']}\")\n",
        "\n",
        "    language, lang_scores = _language_by_regex(code)\n",
        "    return {\n",
        "        \"language\": language,\n",
        "        \"language_scores\": lang_scores,\n",
        "        \"optimization_types\": sorted(set(ctx)),\n",
        "        \"tags\": sorted(set(tags)),\n",
        "        \"kb_rules\": _sanitize_rules(top_rules),   # sanitized\n",
        "        \"inconsistencies\": []\n",
        "    }\n",
        "\n",
        "# ==================================================================================================\n",
        "# Dispatch pour appeler la méthode voulue\n",
        "# ==================================================================================================\n",
        "def run_green_analyzers(code: str, method: str = \"regex\") -> Dict:\n",
        "    method = method.lower()\n",
        "    if method in (\"regex\", \"heuristics\", \"m1\"):\n",
        "        return method1_regex_heuristics(code, knowledge_base)\n",
        "    if method in (\"rf\", \"randomforest\", \"supervised\", \"m4\"):\n",
        "        return method4_supervised_rf(code, knowledge_base)\n",
        "    if method in (\"sbert\", \"kmeans\", \"embed\", \"m5\"):\n",
        "        return method5_embed_cluster(code, knowledge_base)\n",
        "    raise ValueError(\"Unknown method. Use 'regex', 'rf', or 'sbert'.\")\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Run\n",
        "# ------------------------------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    user_code = \"\"\"\n",
        "result = \"\"\n",
        "for i in range(0, len(data)):\n",
        "    result += str(data[i])\n",
        "\"\"\"\n",
        "\n",
        "    print(\"\\n=== Optimizing User Code (Green LLM) ===\\n\")\n",
        "    suggestion = suggest_optimization(user_code)\n",
        "    print(\"\\n\\n=== Final Suggestion ===\\n\")\n",
        "    print(suggestion)\n",
        "\n",
        "    print(\"\\n\\n=== Méthode 1 : Regex + Heuristiques ===\")\n",
        "    print(json.dumps(run_green_analyzers(user_code, \"regex\"), indent=2, ensure_ascii=False))\n",
        "\n",
        "    print(\"\\n=== Méthode 4 : RandomForest supervisé ===\")\n",
        "    try:\n",
        "        print(json.dumps(run_green_analyzers(user_code, \"rf\"), indent=2, ensure_ascii=False))\n",
        "    except RuntimeError as e:\n",
        "        print(\"Méthode 4 indisponible :\", e)\n",
        "\n",
        "    print(\"\\n=== Méthode 5 : SBERT + KMeans ===\")\n",
        "    try:\n",
        "        print(json.dumps(run_green_analyzers(user_code, \"sbert\"), indent=2, ensure_ascii=False))\n",
        "    except RuntimeError as e:\n",
        "        print(\"Méthode 5 indisponible :\", e)\n",
        "\n",
        "    # --- NEW: Show Native AST & Hybrid like in your second slide ---\n",
        "    print(\"\\n=== Analyse AST native ===\")\n",
        "    print(json.dumps(analyze_native_ast(user_code), indent=2, ensure_ascii=False))\n",
        "\n",
        "    print(\"\\n=== Modèle hybride AST + Embeddings ===\")\n",
        "    print(json.dumps(analyze_hybrid_ast_embeddings(user_code, top_k=3), indent=2, ensure_ascii=False))\n",
        "\n",
        "    # --- Mini-benchmark + charts w just 2 ---\n",
        "    try:\n",
        "        import matplotlib.pyplot as plt\n",
        "\n",
        "        # Small demo set (replace with your labeled eval set)\n",
        "        SAMPLES = [\n",
        "            {\n",
        "                \"code\": \"import re\\ns=''\\nfor l in lines:\\n    if re.search(r'\\\\d+', l): s += l\\n\",\n",
        "                \"lang\": \"Python\",\n",
        "            },\n",
        "            {\n",
        "                \"code\": \"import pandas as pd\\ndf['y']=df['x'].apply(lambda v: v*2)\\n\",\n",
        "                \"lang\": \"Python\",\n",
        "            },\n",
        "            {\n",
        "                \"code\": \"function f(x){ console.log(x+1) }\\nimport lib from 'mylib'\\n\",\n",
        "                \"lang\": \"JavaScript\",\n",
        "            },\n",
        "        ]\n",
        "\n",
        "        def evaluate_methods(samples):\n",
        "            methods = {\n",
        "                \"Analyse AST native\": analyze_native_ast,\n",
        "                \"Modèle hybride AST + Embeddings\": lambda c: analyze_hybrid_ast_embeddings(c, top_k=3),\n",
        "            }\n",
        "            tasks = [\"Identification Langage\"]\n",
        "            scores = {t: {m: [] for m in methods} for t in tasks}\n",
        "            for s in samples:\n",
        "                for name, fn in methods.items():\n",
        "                    out = fn(s[\"code\"])\n",
        "                    scores[\"Identification Langage\"][name].append(1.0 if out[\"language\"] == s[\"lang\"] else 0.0)\n",
        "            # average %\n",
        "            return {t: {m: round(100*sum(v)/max(1,len(v)),1) for m, v in d.items()} for t, d in scores.items()}\n",
        "\n",
        "        res = evaluate_methods(SAMPLES)\n",
        "        # plot single chart example (language id)\n",
        "        vals = list(res[\"Identification Langage\"].values())\n",
        "        labels = list(res[\"Identification Langage\"].keys())\n",
        "        fig, ax = plt.subplots(figsize=(8,4))\n",
        "        y = np.arange(len(labels))\n",
        "        ax.barh(y, vals)\n",
        "        ax.set_yticks(y, labels)\n",
        "        ax.set_xlim(0,100)\n",
        "        ax.set_xlabel(\"Taux de précision (%)\")\n",
        "        ax.set_title(\"Identification Langage (%)\")\n",
        "        plt.tight_layout(); plt.show()\n",
        "    except Exception:\n",
        "        pass\n"
      ]
    }
  ]
}